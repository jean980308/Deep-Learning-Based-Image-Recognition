{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Efficient_Nets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CFAR-10"
      ],
      "metadata": {
        "id": "NYXd1f3NQlsM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Y_BFDasDw9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset     = torchvision.datasets.CIFAR10(root='./data' ,train=True,download=True,transform=transform)\n",
        "\n",
        "trainloader   = torch.utils.data.DataLoader(trainset , batch_size=batch_size,shuffle=True , num_workers=2)\n",
        "\n",
        "testset     = torchvision.datasets.CIFAR10(root='./data' ,train=True,download=True , transform=transform)\n",
        "\n",
        "testloader =torch.utils.data.DataLoader(testset , batch_size=batch_size ,shuffle=False ,num_workers=2)\n",
        "\n",
        "classes = ('plane' , 'car' ,'bird' , 'cat' , 'deer' ,'dog' ,'frog' ,'horse' ,'ship' , 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    img= img/2 +0.5\n",
        "    npimg=img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
        "    plt.show()\n",
        "    \n",
        "dataiter= iter(trainloader)\n",
        "images , labels =dataiter.next()\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "print(''.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
      ],
      "metadata": {
        "id": "taK-RIEYsieo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X98P7lGSQint"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lars_optimizer"
      ],
      "metadata": {
        "id": "JbsFk5bfQpdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Layer-wise Adaptive Rate Scaling optimizer for large-batch training.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "class LARSOptimizer(tf.train.Optimizer):\n",
        "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
        "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
        "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
        "  Implements the LARS learning rate scheme presented in the paper above. This\n",
        "  optimizer is useful when scaling the batch size to up to 32K without\n",
        "  significant performance degradation. It is recommended to use the optimizer\n",
        "  in conjunction with:\n",
        "      - Gradual learning rate warm-up\n",
        "      - Linear learning rate scaling\n",
        "      - Poly rule learning rate decay\n",
        "  Note, LARS scaling is currently only enabled for dense tensors. Sparse tensors\n",
        "  use the default momentum optimizer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      learning_rate,\n",
        "      momentum=0.9,\n",
        "      weight_decay=0.0001,\n",
        "      # The LARS coefficient is a hyperparameter\n",
        "      eeta=0.001,\n",
        "      epsilon=0.0,\n",
        "      name=\"LARSOptimizer\",\n",
        "      # Enable skipping variables from LARS scaling.\n",
        "      # TODO(sameerkm): Enable a direct mechanism to pass a\n",
        "      # subset of variables to the optimizer.\n",
        "      skip_list=None,\n",
        "      use_nesterov=False):\n",
        "    \"\"\"Construct a new LARS Optimizer.\n",
        "    Args:\n",
        "      learning_rate: A `Tensor` or floating point value. The base learning rate.\n",
        "      momentum: A floating point value. Momentum hyperparameter.\n",
        "      weight_decay: A floating point value. Weight decay hyperparameter.\n",
        "      eeta: LARS coefficient as used in the paper. Dfault set to LARS\n",
        "        coefficient from the paper. (eeta / weight_decay) determines the highest\n",
        "        scaling factor in LARS.\n",
        "      epsilon: Optional epsilon parameter to be set in models that have very\n",
        "        small gradients. Default set to 0.0.\n",
        "      name: Optional name prefix for variables and ops created by LARSOptimizer.\n",
        "      skip_list: List of strings to enable skipping variables from LARS scaling.\n",
        "        If any of the strings in skip_list is a subset of var.name, variable\n",
        "        'var' is skipped from LARS scaling. For a typical classification model\n",
        "        with batch normalization, the skip_list is ['batch_normalization',\n",
        "        'bias']\n",
        "      use_nesterov: when set to True, nesterov momentum will be enabled\n",
        "    Raises:\n",
        "      ValueError: If a hyperparameter is set to a non-sensical value.\n",
        "    \"\"\"\n",
        "    if momentum < 0.0:\n",
        "      raise ValueError(\"momentum should be positive: %s\" % momentum)\n",
        "    if weight_decay < 0.0:\n",
        "      raise ValueError(\"weight_decay should be positive: %s\" % weight_decay)\n",
        "    super(LARSOptimizer, self).__init__(use_locking=False, name=name)\n",
        "\n",
        "    self._learning_rate = learning_rate\n",
        "    self._momentum = momentum\n",
        "    self._weight_decay = weight_decay\n",
        "    self._eeta = eeta\n",
        "    self._epsilon = epsilon\n",
        "    self._name = name\n",
        "    self._skip_list = skip_list\n",
        "    self._use_nesterov = use_nesterov\n",
        "\n",
        "  def _create_slots(self, var_list):\n",
        "    for v in var_list:\n",
        "      self._zeros_slot(v, \"momentum\", self._name)\n",
        "\n",
        "  def compute_lr(self, grad, var):\n",
        "    scaled_lr = self._learning_rate\n",
        "    if self._skip_list is None or not any(v in var.name\n",
        "                                          for v in self._skip_list):\n",
        "      w_norm = tf.norm(var, ord=2)\n",
        "      g_norm = tf.norm(grad, ord=2)\n",
        "      trust_ratio = tf.where(\n",
        "          tf.math.greater(w_norm, 0),\n",
        "          tf.where(\n",
        "              tf.math.greater(g_norm, 0),\n",
        "              (self._eeta * w_norm /\n",
        "               (g_norm + self._weight_decay * w_norm + self._epsilon)), 1.0),\n",
        "          1.0)\n",
        "      scaled_lr = self._learning_rate * trust_ratio\n",
        "      # Add the weight regularization gradient\n",
        "      grad = grad + self._weight_decay * var\n",
        "    return scaled_lr, grad\n",
        "\n",
        "  def _apply_dense(self, grad, var):\n",
        "    scaled_lr, grad = self.compute_lr(grad, var)\n",
        "    mom = self.get_slot(var, \"momentum\")\n",
        "    return tf.raw_ops.ApplyMomentum(\n",
        "        var,\n",
        "        mom,\n",
        "        tf.cast(1.0, var.dtype.base_dtype),\n",
        "        grad * scaled_lr,\n",
        "        self._momentum,\n",
        "        use_locking=False,\n",
        "        use_nesterov=self._use_nesterov)\n",
        "\n",
        "  def _resource_apply_dense(self, grad, var):\n",
        "    scaled_lr, grad = self.compute_lr(grad, var)\n",
        "    mom = self.get_slot(var, \"momentum\")\n",
        "    return tf.raw_ops.ResourceApplyMomentum(\n",
        "        var=var.handle,\n",
        "        accum=mom.handle,\n",
        "        lr=tf.cast(1.0, var.dtype.base_dtype),\n",
        "        grad=grad * scaled_lr,\n",
        "        momentum=self._momentum,\n",
        "        use_locking=False,\n",
        "        use_nesterov=self._use_nesterov)\n",
        "\n",
        "  # Fallback to momentum optimizer for sparse tensors\n",
        "  def _apply_sparse(self, grad, var):\n",
        "    mom = self.get_slot(var, \"momentum\")\n",
        "    return tf.raw_ops.SparseApplyMomentum(\n",
        "        var,\n",
        "        mom,\n",
        "        tf.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n",
        "        grad.values,\n",
        "        grad.indices,\n",
        "        tf.cast(self._momentum_tensor, var.dtype.base_dtype),\n",
        "        use_locking=self._use_locking,\n",
        "        use_nesterov=self._use_nesterov).op\n",
        "\n",
        "  def _resource_apply_sparse(self, grad, var, indices):\n",
        "    mom = self.get_slot(var, \"momentum\")\n",
        "    return tf.raw_ops.ResourceSparseApplyMomentum(\n",
        "        var.handle,\n",
        "        mom.handle,\n",
        "        tf.cast(self._learning_rate_tensor, grad.dtype),\n",
        "        grad,\n",
        "        indices,\n",
        "        tf.cast(self._momentum_tensor, grad.dtype),\n",
        "        use_locking=self._use_locking,\n",
        "        use_nesterov=self._use_nesterov)\n",
        "\n",
        "  def _prepare(self):\n",
        "    learning_rate = self._learning_rate\n",
        "    if callable(learning_rate):\n",
        "      learning_rate = learning_rate()\n",
        "    self._learning_rate_tensor = tf.convert_to_tensor(\n",
        "        learning_rate, name=\"learning_rate\")\n",
        "    momentum = self._momentum\n",
        "    if callable(momentum):\n",
        "      momentum = momentum()\n",
        "    self._momentum_tensor = tf.convert_to_tensor(momentum, name=\"momentum\")"
      ],
      "metadata": {
        "id": "dhzGpkV-l0dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils"
      ],
      "metadata": {
        "id": "g_I1z0M4QwMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Model utilities.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#import lars_optimizer\n",
        "from tensorflow.python.tpu import tpu_function  # pylint:disable=g-direct-tensorflow-import\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def build_learning_rate(initial_lr,\n",
        "                        global_step,\n",
        "                        steps_per_epoch=None,\n",
        "                        lr_decay_type='exponential',\n",
        "                        decay_factor=0.97,\n",
        "                        decay_epochs=2.4,\n",
        "                        total_steps=None,\n",
        "                        warmup_epochs=5):\n",
        "  \"\"\"Build learning rate.\"\"\"\n",
        "  if lr_decay_type == 'exponential':\n",
        "    assert steps_per_epoch is not None\n",
        "    decay_steps = steps_per_epoch * decay_epochs\n",
        "    lr = tf.train.exponential_decay(\n",
        "        initial_lr, global_step, decay_steps, decay_factor, staircase=True)\n",
        "  elif lr_decay_type == 'cosine':\n",
        "    assert total_steps is not None\n",
        "    lr = 0.5 * initial_lr * (\n",
        "        1 + tf.cos(np.pi * tf.cast(global_step, tf.float32) / total_steps))\n",
        "  elif lr_decay_type == 'constant':\n",
        "    lr = initial_lr\n",
        "  elif lr_decay_type == 'poly':\n",
        "    tf.logging.info('Using poly LR schedule')\n",
        "    assert steps_per_epoch is not None\n",
        "    assert total_steps is not None\n",
        "    warmup_steps = int(steps_per_epoch * warmup_epochs)\n",
        "    min_step = tf.constant(1, dtype=tf.int64)\n",
        "    decay_steps = tf.maximum(min_step, tf.subtract(global_step, warmup_steps))\n",
        "    lr = tf.train.polynomial_decay(\n",
        "        initial_lr,\n",
        "        decay_steps,\n",
        "        total_steps - warmup_steps + 1,\n",
        "        end_learning_rate=0.1,\n",
        "        power=2.0)\n",
        "  else:\n",
        "    assert False, 'Unknown lr_decay_type : %s' % lr_decay_type\n",
        "\n",
        "  if warmup_epochs:\n",
        "    logging.info('Learning rate warmup_epochs: %d', warmup_epochs)\n",
        "    warmup_steps = int(warmup_epochs * steps_per_epoch)\n",
        "    warmup_lr = (\n",
        "        initial_lr * tf.cast(global_step, tf.float32) / tf.cast(\n",
        "            warmup_steps, tf.float32))\n",
        "    lr = tf.cond(global_step < warmup_steps, lambda: warmup_lr, lambda: lr)\n",
        "\n",
        "  return lr\n",
        "\n",
        "\n",
        "def build_optimizer(learning_rate,\n",
        "                    optimizer_name='rmsprop',\n",
        "                    decay=0.9,\n",
        "                    epsilon=0.001,\n",
        "                    momentum=0.9,\n",
        "                    lars_weight_decay=None,\n",
        "                    lars_epsilon=None):\n",
        "  \"\"\"Build optimizer.\"\"\"\n",
        "  if optimizer_name == 'sgd':\n",
        "    logging.info('Using SGD optimizer')\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  elif optimizer_name == 'momentum':\n",
        "    logging.info('Using Momentum optimizer')\n",
        "    optimizer = tf.train.MomentumOptimizer(\n",
        "        learning_rate=learning_rate, momentum=momentum)\n",
        "  elif optimizer_name == 'rmsprop':\n",
        "    logging.info('Using RMSProp optimizer')\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay, momentum,\n",
        "                                          epsilon)\n",
        "  elif optimizer_name == 'lars':\n",
        "    logging.info('Using LARS optimizer')\n",
        "    assert lars_weight_decay is not None, 'LARS weight decay is None.'\n",
        "    assert lars_epsilon is not None, 'LARS epsilon is None.'\n",
        "    optimizer = lars_optimizer.LARSOptimizer(\n",
        "        learning_rate,\n",
        "        momentum=momentum,\n",
        "        weight_decay=lars_weight_decay,\n",
        "        skip_list=['batch_normalization', 'bias', 'beta', 'gamma'],\n",
        "        epsilon=lars_epsilon)\n",
        "  else:\n",
        "    logging.fatal('Unknown optimizer: %s', optimizer_name)\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "class TpuBatchNormalization(tf.layers.BatchNormalization):\n",
        "  # class TpuBatchNormalization(tf.layers.BatchNormalization):\n",
        "  \"\"\"Cross replica batch normalization.\"\"\"\n",
        "\n",
        "  def __init__(self, fused=False, **kwargs):\n",
        "    if fused in (True, None):\n",
        "      raise ValueError('TpuBatchNormalization does not support fused=True.')\n",
        "    super(TpuBatchNormalization, self).__init__(fused=fused, **kwargs)\n",
        "\n",
        "  def _cross_replica_average(self, t, num_shards_per_group):\n",
        "    \"\"\"Calculates the average value of input tensor across TPU replicas.\"\"\"\n",
        "    num_shards = tpu_function.get_tpu_context().number_of_shards\n",
        "    group_assignment = None\n",
        "    if num_shards_per_group > 1:\n",
        "      if num_shards % num_shards_per_group != 0:\n",
        "        raise ValueError('num_shards: %d mod shards_per_group: %d, should be 0'\n",
        "                         % (num_shards, num_shards_per_group))\n",
        "      num_groups = num_shards // num_shards_per_group\n",
        "      group_assignment = [[\n",
        "          x for x in range(num_shards) if x // num_shards_per_group == y\n",
        "      ] for y in range(num_groups)]\n",
        "    return tf.tpu.cross_replica_sum(t, group_assignment) / tf.cast(\n",
        "        num_shards_per_group, t.dtype)\n",
        "\n",
        "  def _moments(self, inputs, reduction_axes, keep_dims):\n",
        "    \"\"\"Compute the mean and variance: it overrides the original _moments.\"\"\"\n",
        "    shard_mean, shard_variance = super(TpuBatchNormalization, self)._moments(\n",
        "        inputs, reduction_axes, keep_dims=keep_dims)\n",
        "\n",
        "    num_shards = tpu_function.get_tpu_context().number_of_shards or 1\n",
        "    if num_shards <= 8:  # Skip cross_replica for 2x2 or smaller slices.\n",
        "      num_shards_per_group = 1\n",
        "    else:\n",
        "      num_shards_per_group = max(8, num_shards // 8)\n",
        "    logging.info('TpuBatchNormalization with num_shards_per_group %s',\n",
        "                 num_shards_per_group)\n",
        "    if num_shards_per_group > 1:\n",
        "      # Compute variance using: Var[X]= E[X^2] - E[X]^2.\n",
        "      shard_square_of_mean = tf.math.square(shard_mean)\n",
        "      shard_mean_of_square = shard_variance + shard_square_of_mean\n",
        "      group_mean = self._cross_replica_average(\n",
        "          shard_mean, num_shards_per_group)\n",
        "      group_mean_of_square = self._cross_replica_average(\n",
        "          shard_mean_of_square, num_shards_per_group)\n",
        "      group_variance = group_mean_of_square - tf.math.square(group_mean)\n",
        "      return (group_mean, group_variance)\n",
        "    else:\n",
        "      return (shard_mean, shard_variance)\n",
        "\n",
        "\n",
        "class BatchNormalization(tf.layers.BatchNormalization):\n",
        "  \"\"\"Fixed default name of BatchNormalization to match TpuBatchNormalization.\"\"\"\n",
        "\n",
        "  def __init__(self, name='tpu_batch_normalization', **kwargs):\n",
        "    super(BatchNormalization, self).__init__(name=name, **kwargs)\n",
        "\n",
        "\n",
        "def train_batch_norm(**kwargs):\n",
        "  if 'optimizer' in FLAGS and FLAGS.optimizer == 'lars':\n",
        "    return DistributedBatchNormalization(**kwargs)\n",
        "  return TpuBatchNormalization(**kwargs)\n",
        "\n",
        "\n",
        "def eval_batch_norm(**kwargs):\n",
        "  if 'optimizer' in FLAGS and FLAGS.optimizer == 'lars':\n",
        "    return DistributedBatchNormalization(**kwargs)\n",
        "  return BatchNormalization(**kwargs)\n",
        "\n",
        "\n",
        "class DistributedBatchNormalization:\n",
        "  \"\"\"Distributed batch normalization used in https://arxiv.org/abs/2011.00071.\"\"\"\n",
        "\n",
        "  def __init__(self, axis, momentum, epsilon):\n",
        "    self.axis = axis\n",
        "    self.momentum = momentum\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def __call__(self, x, training, distname='batch_normalization'):\n",
        "    shape = [x.shape[-1]]\n",
        "    with tf.variable_scope('batch_normalization'):\n",
        "      ones = tf.initializers.ones()\n",
        "      zeros = tf.initializers.zeros()\n",
        "      gamma = tf.get_variable(\n",
        "          'gamma', shape, initializer=ones, trainable=True, use_resource=True)\n",
        "      beta = tf.get_variable(\n",
        "          'beta', shape, initializer=zeros, trainable=True, use_resource=True)\n",
        "      moving_mean = tf.get_variable(\n",
        "          'moving_mean',\n",
        "          shape,\n",
        "          initializer=zeros,\n",
        "          trainable=False,\n",
        "          use_resource=True)\n",
        "      moving_variance = tf.get_variable(\n",
        "          'moving_variance',\n",
        "          shape,\n",
        "          initializer=ones,\n",
        "          trainable=False,\n",
        "          use_resource=True)\n",
        "    num_replicas = FLAGS.num_replicas\n",
        "\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    if training:\n",
        "      if num_replicas <= 8:\n",
        "        group_assign = None\n",
        "        group_shards = tf.cast(num_replicas, tf.float32)\n",
        "      else:\n",
        "\n",
        "        group_shards = max(\n",
        "            1,\n",
        "            int(FLAGS.batch_norm_batch_size /\n",
        "                (FLAGS.train_batch_size / num_replicas)))\n",
        "        group_assign = np.arange(num_replicas, dtype=np.int32)\n",
        "        group_assign = group_assign.reshape([-1, group_shards])\n",
        "        group_assign = group_assign.tolist()\n",
        "        group_shards = tf.cast(group_shards, tf.float32)\n",
        "\n",
        "      mean = tf.reduce_mean(x, [0, 1, 2])\n",
        "      mean = tf.tpu.cross_replica_sum(mean, group_assign) / group_shards\n",
        "\n",
        "      # Var[x] = E[x^2] - E[x]^2\n",
        "      mean_sq = tf.reduce_mean(tf.math.square(x), [0, 1, 2])\n",
        "      mean_sq = tf.tpu.cross_replica_sum(mean_sq, group_assign) / group_shards\n",
        "      variance = mean_sq - tf.math.square(mean)\n",
        "\n",
        "      decay = tf.cast(1. - self.momentum, tf.float32)\n",
        "\n",
        "      def u(moving, normal, name):\n",
        "        num_replicas_fp = tf.cast(num_replicas, tf.float32)\n",
        "        normal = tf.tpu.cross_replica_sum(normal) / num_replicas_fp\n",
        "        diff = decay * (moving - normal)\n",
        "        return tf.assign_sub(moving, diff, use_locking=True, name=name)\n",
        "\n",
        "      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                           u(moving_mean, mean, name='moving_mean'))\n",
        "      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
        "                           u(moving_variance, variance, name='moving_variance'))\n",
        "\n",
        "      x = tf.nn.batch_normalization(\n",
        "          x,\n",
        "          mean=mean,\n",
        "          variance=variance,\n",
        "          offset=beta,\n",
        "          scale=gamma,\n",
        "          variance_epsilon=self.epsilon)\n",
        "    else:\n",
        "\n",
        "      x, _, _ = tf.nn.fused_batch_norm(\n",
        "          x,\n",
        "          scale=gamma,\n",
        "          offset=beta,\n",
        "          mean=moving_mean,\n",
        "          variance=moving_variance,\n",
        "          epsilon=self.epsilon,\n",
        "          is_training=False)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_connect(inputs, is_training, survival_prob):\n",
        "  \"\"\"Drop the entire conv with given survival probability.\"\"\"\n",
        "  # \"Deep Networks with Stochastic Depth\", https://arxiv.org/pdf/1603.09382.pdf\n",
        "  if not is_training:\n",
        "    return inputs\n",
        "\n",
        "  # Compute tensor.\n",
        "  batch_size = tf.shape(inputs)[0]\n",
        "  random_tensor = survival_prob\n",
        "  random_tensor += tf.random_uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n",
        "  binary_tensor = tf.floor(random_tensor)\n",
        "  # Unlike conventional way that multiply survival_prob at test time, here we\n",
        "  # divide survival_prob at training time, such that no addition compute is\n",
        "  # needed at test time.\n",
        "  output = tf.div(inputs, survival_prob) * binary_tensor\n",
        "  return output\n",
        "\n",
        "\n",
        "def archive_ckpt(ckpt_eval, ckpt_objective, ckpt_path):\n",
        "  \"\"\"Archive a checkpoint if the metric is better.\"\"\"\n",
        "  ckpt_dir, ckpt_name = os.path.split(ckpt_path)\n",
        "\n",
        "  saved_objective_path = os.path.join(ckpt_dir, 'best_objective.txt')\n",
        "  saved_objective = float('-inf')\n",
        "  if tf.gfile.Exists(saved_objective_path):\n",
        "    with tf.gfile.GFile(saved_objective_path, 'r') as f:\n",
        "      saved_objective = float(f.read())\n",
        "  if saved_objective > ckpt_objective:\n",
        "    logging.info('Ckpt %s is worse than %s', ckpt_objective, saved_objective)\n",
        "    return False\n",
        "\n",
        "  filenames = tf.gfile.Glob(ckpt_path + '.*')\n",
        "  if filenames is None:\n",
        "    logging.info('No files to copy for checkpoint %s', ckpt_path)\n",
        "    return False\n",
        "\n",
        "  # Clear the old folder.\n",
        "  dst_dir = os.path.join(ckpt_dir, 'archive')\n",
        "  if tf.gfile.Exists(dst_dir):\n",
        "    tf.gfile.DeleteRecursively(dst_dir)\n",
        "  tf.gfile.MakeDirs(dst_dir)\n",
        "\n",
        "  # Write checkpoints.\n",
        "  for f in filenames:\n",
        "    dest = os.path.join(dst_dir, os.path.basename(f))\n",
        "    tf.gfile.Copy(f, dest, overwrite=True)\n",
        "  ckpt_state = tf.train.generate_checkpoint_state_proto(\n",
        "      dst_dir,\n",
        "      model_checkpoint_path=ckpt_name,\n",
        "      all_model_checkpoint_paths=[ckpt_name])\n",
        "  with tf.gfile.GFile(os.path.join(dst_dir, 'checkpoint'), 'w') as f:\n",
        "    f.write(str(ckpt_state))\n",
        "  with tf.gfile.GFile(os.path.join(dst_dir, 'best_eval.txt'), 'w') as f:\n",
        "    f.write('%s' % ckpt_eval)\n",
        "\n",
        "  # Update the best objective.\n",
        "  with tf.gfile.GFile(saved_objective_path, 'w') as f:\n",
        "    f.write('%f' % ckpt_objective)\n",
        "\n",
        "  logging.info('Copying checkpoint %s to %s', ckpt_path, dst_dir)\n",
        "  return True\n",
        "\n",
        "\n",
        "def get_ema_vars():\n",
        "  \"\"\"Get all exponential moving average (ema) variables.\"\"\"\n",
        "  ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\n",
        "  for v in tf.global_variables():\n",
        "    # We maintain mva for batch norm moving mean and variance as well.\n",
        "    if 'moving_mean' in v.name or 'moving_variance' in v.name:\n",
        "      ema_vars.append(v)\n",
        "  return list(set(ema_vars))\n",
        "\n",
        "\n",
        "class DepthwiseConv2D(tf.keras.layers.DepthwiseConv2D, tf.layers.Layer):\n",
        "  \"\"\"Wrap keras DepthwiseConv2D to tf.layers.\"\"\"\n",
        "\n",
        "  pass\n",
        "\n",
        "\n",
        "class Conv2D(tf.layers.Conv2D):\n",
        "  \"\"\"Wrapper for Conv2D with specialization for fast inference.\"\"\"\n",
        "\n",
        "  def _bias_activation(self, outputs):\n",
        "    if self.use_bias:\n",
        "      outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "    if self.activation is not None:\n",
        "      return self.activation(outputs)\n",
        "    return outputs\n",
        "\n",
        "  def _can_run_fast_1x1(self, inputs):\n",
        "    batch_size = inputs.shape.as_list()[0]\n",
        "    return (self.data_format == 'channels_first' and\n",
        "            batch_size == 1 and\n",
        "            self.kernel_size == (1, 1))\n",
        "\n",
        "  def _call_fast_1x1(self, inputs):\n",
        "    # Compute the 1x1 convolution as a matmul.\n",
        "    inputs_shape = tf.shape(inputs)\n",
        "    flat_inputs = tf.reshape(inputs, [inputs_shape[1], -1])\n",
        "    flat_outputs = tf.matmul(\n",
        "        tf.squeeze(self.kernel),\n",
        "        flat_inputs,\n",
        "        transpose_a=True)\n",
        "    outputs_shape = tf.concat([[1, self.filters], inputs_shape[2:]], axis=0)\n",
        "    outputs = tf.reshape(flat_outputs, outputs_shape)\n",
        "\n",
        "    # Handle the bias and activation function.\n",
        "    return self._bias_activation(outputs)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if self._can_run_fast_1x1(inputs):\n",
        "      return self._call_fast_1x1(inputs)\n",
        "    return super(Conv2D, self).call(inputs)\n",
        "\n",
        "\n",
        "class EvalCkptDriver(object):\n",
        "  \"\"\"A driver for running eval inference.\n",
        "  Attributes:\n",
        "    model_name: str. Model name to eval.\n",
        "    batch_size: int. Eval batch size.\n",
        "    image_size: int. Input image size, determined by model name.\n",
        "    num_classes: int. Number of classes, default to 1000 for ImageNet.\n",
        "    include_background_label: whether to include extra background label.\n",
        "    advprop_preprocessing: whether to use advprop preprocessing.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               model_name,\n",
        "               batch_size=1,\n",
        "               image_size=224,\n",
        "               num_classes=1000,\n",
        "               include_background_label=False,\n",
        "               advprop_preprocessing=False):\n",
        "    \"\"\"Initialize internal variables.\"\"\"\n",
        "    self.model_name = model_name\n",
        "    self.batch_size = batch_size\n",
        "    self.num_classes = num_classes\n",
        "    self.include_background_label = include_background_label\n",
        "    self.image_size = image_size\n",
        "    self.advprop_preprocessing = advprop_preprocessing\n",
        "\n",
        "  def restore_model(self, sess, ckpt_dir, enable_ema=True, export_ckpt=None):\n",
        "    \"\"\"Restore variables from checkpoint dir.\"\"\"\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
        "    if enable_ema:\n",
        "      ema = tf.train.ExponentialMovingAverage(decay=0.0)\n",
        "      ema_vars = get_ema_vars()\n",
        "      var_dict = ema.variables_to_restore(ema_vars)\n",
        "      ema_assign_op = ema.apply(ema_vars)\n",
        "    else:\n",
        "      var_dict = get_ema_vars()\n",
        "      ema_assign_op = None\n",
        "\n",
        "    tf.train.get_or_create_global_step()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(var_dict, max_to_keep=1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "\n",
        "    if export_ckpt:\n",
        "      if ema_assign_op is not None:\n",
        "        sess.run(ema_assign_op)\n",
        "      saver = tf.train.Saver(max_to_keep=1, save_relative_paths=True)\n",
        "      saver.save(sess, export_ckpt)\n",
        "\n",
        "  def build_model(self, features, is_training):\n",
        "    \"\"\"Build model with input features.\"\"\"\n",
        "    del features, is_training\n",
        "    raise ValueError('Must be implemented by subclasses.')\n",
        "\n",
        "  def get_preprocess_fn(self):\n",
        "    raise ValueError('Must be implemented by subclsses.')\n",
        "\n",
        "  def build_dataset(self, filenames, labels, is_training):\n",
        "    \"\"\"Build input dataset.\"\"\"\n",
        "    batch_drop_remainder = False\n",
        "    if 'condconv' in self.model_name and not is_training:\n",
        "      # CondConv layers can only be called with known batch dimension. Thus, we\n",
        "      # must drop all remaining examples that do not make up one full batch.\n",
        "      # To ensure all examples are evaluated, use a batch size that evenly\n",
        "      # divides the number of files.\n",
        "      batch_drop_remainder = True\n",
        "      num_files = len(filenames)\n",
        "      if num_files % self.batch_size != 0:\n",
        "        tf.logging.warn('Remaining examples in last batch are not being '\n",
        "                        'evaluated.')\n",
        "    filenames = tf.constant(filenames)\n",
        "    labels = tf.constant(labels)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    def _parse_function(filename, label):\n",
        "      image_string = tf.read_file(filename)\n",
        "      preprocess_fn = self.get_preprocess_fn()\n",
        "      image_decoded = preprocess_fn(\n",
        "          image_string, is_training, image_size=self.image_size)\n",
        "      image = tf.cast(image_decoded, tf.float32)\n",
        "      return image, label\n",
        "\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    dataset = dataset.batch(self.batch_size,\n",
        "                            drop_remainder=batch_drop_remainder)\n",
        "\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    images, labels = iterator.get_next()\n",
        "    return images, labels\n",
        "\n",
        "  def run_inference(self,\n",
        "                    ckpt_dir,\n",
        "                    image_files,\n",
        "                    labels,\n",
        "                    enable_ema=True,\n",
        "                    export_ckpt=None):\n",
        "    \"\"\"Build and run inference on the target images and labels.\"\"\"\n",
        "    label_offset = 1 if self.include_background_label else 0\n",
        "    with tf.Graph().as_default(), tf.Session() as sess:\n",
        "      images, labels = self.build_dataset(image_files, labels, False)\n",
        "      probs = self.build_model(images, is_training=False)\n",
        "      if isinstance(probs, tuple):\n",
        "        probs = probs[0]\n",
        "\n",
        "      self.restore_model(sess, ckpt_dir, enable_ema, export_ckpt)\n",
        "\n",
        "      prediction_idx = []\n",
        "      prediction_prob = []\n",
        "      for _ in range(len(image_files) // self.batch_size):\n",
        "        out_probs = sess.run(probs)\n",
        "        idx = np.argsort(out_probs)[::-1]\n",
        "        prediction_idx.append(idx[:5] - label_offset)\n",
        "        prediction_prob.append([out_probs[pid] for pid in idx[:5]])\n",
        "\n",
        "      # Return the top 5 predictions (idx and prob) for each image.\n",
        "      return prediction_idx, prediction_prob\n",
        "\n",
        "  def eval_example_images(self,\n",
        "                          ckpt_dir,\n",
        "                          image_files,\n",
        "                          labels_map_file,\n",
        "                          enable_ema=True,\n",
        "                          export_ckpt=None):\n",
        "    \"\"\"Eval a list of example images.\n",
        "    Args:\n",
        "      ckpt_dir: str. Checkpoint directory path.\n",
        "      image_files: List[str]. A list of image file paths.\n",
        "      labels_map_file: str. The labels map file path.\n",
        "      enable_ema: enable expotential moving average.\n",
        "      export_ckpt: export ckpt folder.\n",
        "    Returns:\n",
        "      A tuple (pred_idx, and pred_prob), where pred_idx is the top 5 prediction\n",
        "      index and pred_prob is the top 5 prediction probability.\n",
        "    \"\"\"\n",
        "    classes = json.loads(tf.gfile.Open(labels_map_file).read())\n",
        "    pred_idx, pred_prob = self.run_inference(\n",
        "        ckpt_dir, image_files, [0] * len(image_files), enable_ema, export_ckpt)\n",
        "    for i in range(len(image_files)):\n",
        "      print('predicted class for image {}: '.format(image_files[i]))\n",
        "      for j, idx in enumerate(pred_idx[i]):\n",
        "        print('  -> top_{} ({:4.2f}%): {}  '.format(j, pred_prob[i][j] * 100,\n",
        "                                                    classes[str(idx)]))\n",
        "    return pred_idx, pred_prob\n",
        "\n",
        "  def eval_imagenet(self, ckpt_dir, imagenet_eval_glob,\n",
        "                    imagenet_eval_label, num_images, enable_ema, export_ckpt):\n",
        "    \"\"\"Eval ImageNet images and report top1/top5 accuracy.\n",
        "    Args:\n",
        "      ckpt_dir: str. Checkpoint directory path.\n",
        "      imagenet_eval_glob: str. File path glob for all eval images.\n",
        "      imagenet_eval_label: str. File path for eval label.\n",
        "      num_images: int. Number of images to eval: -1 means eval the whole\n",
        "        dataset.\n",
        "      enable_ema: enable expotential moving average.\n",
        "      export_ckpt: export checkpoint folder.\n",
        "    Returns:\n",
        "      A tuple (top1, top5) for top1 and top5 accuracy.\n",
        "    \"\"\"\n",
        "    imagenet_val_labels = [int(i) for i in tf.gfile.GFile(imagenet_eval_label)]\n",
        "    imagenet_filenames = sorted(tf.gfile.Glob(imagenet_eval_glob))\n",
        "    if num_images < 0:\n",
        "      num_images = len(imagenet_filenames)\n",
        "    image_files = imagenet_filenames[:num_images]\n",
        "    labels = imagenet_val_labels[:num_images]\n",
        "\n",
        "    pred_idx, _ = self.run_inference(\n",
        "        ckpt_dir, image_files, labels, enable_ema, export_ckpt)\n",
        "    top1_cnt, top5_cnt = 0.0, 0.0\n",
        "    for i, label in enumerate(labels):\n",
        "      top1_cnt += label in pred_idx[i][:1]\n",
        "      top5_cnt += label in pred_idx[i][:5]\n",
        "      if i % 100 == 0:\n",
        "        print('Step {}: top1_acc = {:4.2f}%  top5_acc = {:4.2f}%'.format(\n",
        "            i, 100 * top1_cnt / (i + 1), 100 * top5_cnt / (i + 1)))\n",
        "        sys.stdout.flush()\n",
        "    top1, top5 = 100 * top1_cnt / num_images, 100 * top5_cnt / num_images\n",
        "    print('Final: top1_acc = {:4.2f}%  top5_acc = {:4.2f}%'.format(top1, top5))\n",
        "    return top1, top5"
      ],
      "metadata": {
        "id": "JrRodLCIi4RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# condconv_layers"
      ],
      "metadata": {
        "id": "nQ0snsmyQ1Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"CondConv implementations in Tensorflow Layers.\n",
        "[1] Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam\n",
        "  CondConv: Conditionally Parameterized Convolutions for Efficient Inference.\n",
        "  NeurIPS'19, https://arxiv.org/abs/1904.04971\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "def get_condconv_initializer(initializer, num_experts, expert_shape):\n",
        "  \"\"\"Wraps the initializer to correctly initialize CondConv variables.\n",
        "  CondConv initializes biases and kernels in a num_experts x num_params\n",
        "  matrix for efficient computation. This wrapper ensures that each expert\n",
        "  is correctly initialized with the given initializer before being flattened\n",
        "  into the correctly shaped CondConv variable.\n",
        "  Arguments:\n",
        "    initializer: The initializer to apply for each individual expert.\n",
        "    num_experts: The number of experts to be initialized.\n",
        "    expert_shape: The original shape of each individual expert.\n",
        "  Returns:\n",
        "    The initializer for the num_experts x num_params CondConv variable.\n",
        "  \"\"\"\n",
        "  def condconv_initializer(expected_shape, dtype=None, partition=None):\n",
        "    \"\"\"CondConv initializer function.\"\"\"\n",
        "    num_params = np.prod(expert_shape)\n",
        "    if (len(expected_shape) != 2 or expected_shape[0] != num_experts or\n",
        "        expected_shape[1] != num_params):\n",
        "      raise (ValueError(\n",
        "          'CondConv variables must have shape [num_experts, num_params]'))\n",
        "    flattened_kernels = []\n",
        "    for _ in range(num_experts):\n",
        "      kernel = initializer(expert_shape, dtype, partition)\n",
        "      flattened_kernels.append(tf.reshape(kernel, [-1]))\n",
        "    return tf.stack(flattened_kernels)\n",
        "\n",
        "  return condconv_initializer\n",
        "\n",
        "\n",
        "class CondConv2D(tf.keras.layers.Conv2D):\n",
        "  \"\"\"2D conditional convolution layer (e.g. spatial convolution over images).\n",
        "  Attributes:\n",
        "    filters: Integer, the dimensionality of the output space (i.e. the number of\n",
        "      output filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n",
        "      and width of the 2D convolution window. Can be a single integer to specify\n",
        "      the same value for all spatial dimensions.\n",
        "    num_experts: The number of expert kernels and biases in the CondConv layer.\n",
        "    strides: An integer or tuple/list of 2 integers, specifying the strides of\n",
        "      the convolution along the height and width. Can be a single integer to\n",
        "      specify the same value for all spatial dimensions. Specifying any stride\n",
        "      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n",
        "    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
        "    data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
        "      The ordering of the dimensions in the inputs. `channels_last` corresponds\n",
        "      to inputs with shape `(batch, height, width, channels)` while\n",
        "      `channels_first` corresponds to inputs with shape `(batch, channels,\n",
        "      height, width)`. It defaults to the `image_data_format` value found in\n",
        "      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
        "      it will be \"channels_last\".\n",
        "    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n",
        "      dilation rate to use for dilated convolution. Can be a single integer to\n",
        "      specify the same value for all spatial dimensions. Currently, specifying\n",
        "      any `dilation_rate` value != 1 is incompatible with specifying any stride\n",
        "      value != 1.\n",
        "    activation: Activation function to use. If you don't specify anything, no\n",
        "      activation is applied\n",
        "      (ie. \"linear\" activation: `a(x) = x`).\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
        "      matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    activity_regularizer: Regularizer function applied to the output of the\n",
        "      layer (its \"activation\")..\n",
        "    kernel_constraint: Constraint function applied to the kernel matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "  Input shape:\n",
        "    4D tensor with shape: `(samples, channels, rows, cols)` if\n",
        "      data_format='channels_first'\n",
        "    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n",
        "      data_format='channels_last'.\n",
        "  Output shape:\n",
        "    4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n",
        "      data_format='channels_first'\n",
        "    or 4D tensor with shape: `(samples, new_rows, new_cols, filters)` if\n",
        "      data_format='channels_last'. `rows` and `cols` values might have changed\n",
        "      due to padding.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               filters,\n",
        "               kernel_size,\n",
        "               num_experts,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               data_format=None,\n",
        "               dilation_rate=(1, 1),\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(CondConv2D, self).__init__(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        data_format=data_format,\n",
        "        dilation_rate=dilation_rate,\n",
        "        activation=activation,\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        bias_initializer=bias_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        bias_regularizer=bias_regularizer,\n",
        "        activity_regularizer=activity_regularizer,\n",
        "        kernel_constraint=kernel_constraint,\n",
        "        bias_constraint=bias_constraint,\n",
        "        **kwargs)\n",
        "    if num_experts < 1:\n",
        "      raise ValueError('A CondConv layer must have at least one expert.')\n",
        "    self.num_experts = num_experts\n",
        "    if self.data_format == 'channels_first':\n",
        "      self.converted_data_format = 'NCHW'\n",
        "    else:\n",
        "      self.converted_data_format = 'NHWC'\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    if len(input_shape) != 4:\n",
        "      raise ValueError(\n",
        "          'Inputs to `CondConv2D` should have rank 4. '\n",
        "          'Received input shape:', str(input_shape))\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    channel_axis = self._get_channel_axis()\n",
        "    if input_shape.dims[channel_axis].value is None:\n",
        "      raise ValueError('The channel dimension of the inputs '\n",
        "                       'should be defined. Found `None`.')\n",
        "    input_dim = int(input_shape[channel_axis])\n",
        "\n",
        "    self.kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "    kernel_num_params = 1\n",
        "    for kernel_dim in self.kernel_shape:\n",
        "      kernel_num_params *= kernel_dim\n",
        "    condconv_kernel_shape = (self.num_experts, kernel_num_params)\n",
        "    self.condconv_kernel = self.add_weight(\n",
        "        name='condconv_kernel',\n",
        "        shape=condconv_kernel_shape,\n",
        "        initializer=get_condconv_initializer(self.kernel_initializer,\n",
        "                                             self.num_experts,\n",
        "                                             self.kernel_shape),\n",
        "        regularizer=self.kernel_regularizer,\n",
        "        constraint=self.kernel_constraint,\n",
        "        trainable=True,\n",
        "        dtype=self.dtype)\n",
        "\n",
        "    if self.use_bias:\n",
        "      self.bias_shape = (self.filters,)\n",
        "      condconv_bias_shape = (self.num_experts, self.filters)\n",
        "      self.condconv_bias = self.add_weight(\n",
        "          name='condconv_bias',\n",
        "          shape=condconv_bias_shape,\n",
        "          initializer=get_condconv_initializer(self.bias_initializer,\n",
        "                                               self.num_experts,\n",
        "                                               self.bias_shape),\n",
        "          regularizer=self.bias_regularizer,\n",
        "          constraint=self.bias_constraint,\n",
        "          trainable=True,\n",
        "          dtype=self.dtype)\n",
        "    else:\n",
        "      self.bias = None\n",
        "\n",
        "    self.input_spec = tf.layers.InputSpec(\n",
        "        ndim=self.rank + 2, axes={channel_axis: input_dim})\n",
        "\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, routing_weights):\n",
        "    # Compute example dependent kernels\n",
        "    kernels = tf.matmul(routing_weights, self.condconv_kernel)\n",
        "    batch_size = inputs.shape[0].value\n",
        "    inputs = tf.split(inputs, batch_size, 0)\n",
        "    kernels = tf.split(kernels, batch_size, 0)\n",
        "    # Apply example-dependent convolution to each example in the batch\n",
        "    outputs_list = []\n",
        "    for input_tensor, kernel in zip(inputs, kernels):\n",
        "      kernel = tf.reshape(kernel, self.kernel_shape)\n",
        "      outputs_list.append(\n",
        "          tf.nn.convolution(\n",
        "              input_tensor,\n",
        "              kernel,\n",
        "              strides=self.strides,\n",
        "              padding=self._get_padding_op(),\n",
        "              dilations=self.dilation_rate,\n",
        "              data_format=self.converted_data_format))\n",
        "    outputs = tf.concat(outputs_list, 0)\n",
        "\n",
        "    if self.use_bias:\n",
        "      # Compute example-dependent biases\n",
        "      biases = tf.matmul(routing_weights, self.condconv_bias)\n",
        "      outputs = tf.split(outputs, batch_size, 0)\n",
        "      biases = tf.split(biases, batch_size, 0)\n",
        "      # Add example-dependent bias to each example in the batch\n",
        "      bias_outputs_list = []\n",
        "      for output, bias in zip(outputs, biases):\n",
        "        bias = tf.squeeze(bias, axis=0)\n",
        "        bias_outputs_list.append(\n",
        "            tf.nn.bias_add(output, bias,\n",
        "                           data_format=self.converted_data_format))\n",
        "      outputs = tf.concat(bias_outputs_list, 0)\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(outputs)\n",
        "    return outputs\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {'num_experts': self.num_experts}\n",
        "    base_config = super(CondConv2D, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  def _get_channel_axis(self):\n",
        "    if self.data_format == 'channels_first':\n",
        "      return 1\n",
        "    else:\n",
        "      return -1\n",
        "\n",
        "  def _get_padding_op(self):\n",
        "    if self.padding == 'causal':\n",
        "      op_padding = 'valid'\n",
        "    else:\n",
        "      op_padding = self.padding\n",
        "    if not isinstance(op_padding, (list, tuple)):\n",
        "      op_padding = op_padding.upper()\n",
        "    return op_padding\n",
        "\n",
        "\n",
        "class DepthwiseCondConv2D(tf.keras.layers.DepthwiseConv2D):\n",
        "  \"\"\"Depthwise separable 2D conditional convolution layer.\n",
        "  This layer extends the base depthwise 2D convolution layer to compute\n",
        "  example-dependent parameters. A DepthwiseCondConv2D layer has 'num_experts`\n",
        "  kernels and biases. It computes a kernel and bias for each example as a\n",
        "  weighted sum of experts using the input example-dependent routing weights,\n",
        "  then applies the depthwise convolution to each example.\n",
        "  Attributes:\n",
        "    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n",
        "      and width of the 2D convolution window. Can be a single integer to specify\n",
        "      the same value for all spatial dimensions.\n",
        "    num_experts: The number of expert kernels and biases in the\n",
        "      DepthwiseCondConv2D layer.\n",
        "    strides: An integer or tuple/list of 2 integers, specifying the strides of\n",
        "      the convolution along the height and width. Can be a single integer to\n",
        "      specify the same value for all spatial dimensions. Specifying any stride\n",
        "      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n",
        "    padding: one of `'valid'` or `'same'` (case-insensitive).\n",
        "    depth_multiplier: The number of depthwise convolution output channels for\n",
        "      each input channel. The total number of depthwise convolution output\n",
        "      channels will be equal to `filters_in * depth_multiplier`.\n",
        "    data_format: A string, one of `channels_last` (default) or `channels_first`.\n",
        "      The ordering of the dimensions in the inputs. `channels_last` corresponds\n",
        "      to inputs with shape `(batch, height, width, channels)` while\n",
        "      `channels_first` corresponds to inputs with shape `(batch, channels,\n",
        "      height, width)`. It defaults to the `image_data_format` value found in\n",
        "      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n",
        "      it will be 'channels_last'.\n",
        "    activation: Activation function to use. If you don't specify anything, no\n",
        "      activation is applied\n",
        "      (ie. 'linear' activation: `a(x) = x`).\n",
        "    use_bias: Boolean, whether the layer uses a bias vector.\n",
        "    depthwise_initializer: Initializer for the depthwise kernel matrix.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n",
        "      matrix.\n",
        "    bias_regularizer: Regularizer function applied to the bias vector.\n",
        "    activity_regularizer: Regularizer function applied to the output of the\n",
        "      layer (its 'activation').\n",
        "    depthwise_constraint: Constraint function applied to the depthwise kernel\n",
        "      matrix.\n",
        "    bias_constraint: Constraint function applied to the bias vector.\n",
        "  Input shape:\n",
        "    4D tensor with shape: `[batch, channels, rows, cols]` if\n",
        "      data_format='channels_first'\n",
        "    or 4D tensor with shape: `[batch, rows, cols, channels]` if\n",
        "      data_format='channels_last'.\n",
        "  Output shape:\n",
        "    4D tensor with shape: `[batch, filters, new_rows, new_cols]` if\n",
        "      data_format='channels_first'\n",
        "    or 4D tensor with shape: `[batch, new_rows, new_cols, filters]` if\n",
        "      data_format='channels_last'. `rows` and `cols` values might have changed\n",
        "      due to padding.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               kernel_size,\n",
        "               num_experts,\n",
        "               strides=(1, 1),\n",
        "               padding='valid',\n",
        "               depth_multiplier=1,\n",
        "               data_format=None,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               depthwise_initializer='glorot_uniform',\n",
        "               bias_initializer='zeros',\n",
        "               depthwise_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               depthwise_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(DepthwiseCondConv2D, self).__init__(\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        depth_multiplier=depth_multiplier,\n",
        "        data_format=data_format,\n",
        "        activation=activation,\n",
        "        use_bias=use_bias,\n",
        "        depthwise_initializer=depthwise_initializer,\n",
        "        bias_initializer=bias_initializer,\n",
        "        depthwise_regularizer=depthwise_regularizer,\n",
        "        bias_regularizer=bias_regularizer,\n",
        "        activity_regularizer=activity_regularizer,\n",
        "        depthwise_constraint=depthwise_constraint,\n",
        "        bias_constraint=bias_constraint,\n",
        "        **kwargs)\n",
        "    if num_experts < 1:\n",
        "      raise ValueError('A CondConv layer must have at least one expert.')\n",
        "    self.num_experts = num_experts\n",
        "    if self.data_format == 'channels_first':\n",
        "      self.converted_data_format = 'NCHW'\n",
        "    else:\n",
        "      self.converted_data_format = 'NHWC'\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    if len(input_shape) < 4:\n",
        "      raise ValueError(\n",
        "          'Inputs to `DepthwiseCondConv2D` should have rank 4. '\n",
        "          'Received input shape:', str(input_shape))\n",
        "    input_shape = tf.TensorShape(input_shape)\n",
        "    if self.data_format == 'channels_first':\n",
        "      channel_axis = 1\n",
        "    else:\n",
        "      channel_axis = 3\n",
        "    if input_shape.dims[channel_axis].value is None:\n",
        "      raise ValueError('The channel dimension of the inputs to '\n",
        "                       '`DepthwiseConv2D` '\n",
        "                       'should be defined. Found `None`.')\n",
        "    input_dim = int(input_shape[channel_axis])\n",
        "    self.depthwise_kernel_shape = (self.kernel_size[0], self.kernel_size[1],\n",
        "                                   input_dim, self.depth_multiplier)\n",
        "\n",
        "    depthwise_kernel_num_params = 1\n",
        "    for dim in self.depthwise_kernel_shape:\n",
        "      depthwise_kernel_num_params *= dim\n",
        "    depthwise_condconv_kernel_shape = (self.num_experts,\n",
        "                                       depthwise_kernel_num_params)\n",
        "\n",
        "    self.depthwise_condconv_kernel = self.add_weight(\n",
        "        shape=depthwise_condconv_kernel_shape,\n",
        "        initializer=get_condconv_initializer(self.depthwise_initializer,\n",
        "                                             self.num_experts,\n",
        "                                             self.depthwise_kernel_shape),\n",
        "        name='depthwise_condconv_kernel',\n",
        "        regularizer=self.depthwise_regularizer,\n",
        "        constraint=self.depthwise_constraint,\n",
        "        trainable=True)\n",
        "\n",
        "    if self.use_bias:\n",
        "      bias_dim = input_dim * self.depth_multiplier\n",
        "      self.bias_shape = (bias_dim,)\n",
        "      condconv_bias_shape = (self.num_experts, bias_dim)\n",
        "      self.condconv_bias = self.add_weight(\n",
        "          name='condconv_bias',\n",
        "          shape=condconv_bias_shape,\n",
        "          initializer=get_condconv_initializer(self.bias_initializer,\n",
        "                                               self.num_experts,\n",
        "                                               self.bias_shape),\n",
        "          regularizer=self.bias_regularizer,\n",
        "          constraint=self.bias_constraint,\n",
        "          trainable=True,\n",
        "          dtype=self.dtype)\n",
        "    else:\n",
        "      self.bias = None\n",
        "    # Set input spec.\n",
        "    self.input_spec = tf.layers.InputSpec(\n",
        "        ndim=4, axes={channel_axis: input_dim})\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, inputs, routing_weights):\n",
        "    # Compute example dependent depthwise kernels\n",
        "    depthwise_kernels = tf.matmul(routing_weights,\n",
        "                                  self.depthwise_condconv_kernel)\n",
        "    batch_size = inputs.shape[0].value\n",
        "    inputs = tf.split(inputs, batch_size, 0)\n",
        "    depthwise_kernels = tf.split(depthwise_kernels, batch_size, 0)\n",
        "    # Apply example-dependent depthwise convolution to each example in the batch\n",
        "    outputs_list = []\n",
        "    for input_tensor, depthwise_kernel in zip(inputs, depthwise_kernels):\n",
        "      depthwise_kernel = tf.reshape(depthwise_kernel,\n",
        "                                    self.depthwise_kernel_shape)\n",
        "      if self.data_format == 'channels_first':\n",
        "        converted_strides = (1, 1) + self.strides\n",
        "      else:\n",
        "        converted_strides = (1,) + self.strides + (1,)\n",
        "      outputs_list.append(\n",
        "          tf.nn.depthwise_conv2d(\n",
        "              input_tensor,\n",
        "              depthwise_kernel,\n",
        "              strides=converted_strides,\n",
        "              padding=self.padding.upper(),\n",
        "              dilations=self.dilation_rate,\n",
        "              data_format=self.converted_data_format))\n",
        "    outputs = tf.concat(outputs_list, 0)\n",
        "\n",
        "    if self.use_bias:\n",
        "      # Compute example-dependent biases\n",
        "      biases = tf.matmul(routing_weights, self.condconv_bias)\n",
        "      outputs = tf.split(outputs, batch_size, 0)\n",
        "      biases = tf.split(biases, batch_size, 0)\n",
        "      # Add example-dependent bias to each example in the batch\n",
        "      bias_outputs_list = []\n",
        "      for output, bias in zip(outputs, biases):\n",
        "        bias = tf.squeeze(bias, axis=0)\n",
        "        bias_outputs_list.append(\n",
        "            tf.nn.bias_add(output, bias,\n",
        "                           data_format=self.converted_data_format))\n",
        "      outputs = tf.concat(bias_outputs_list, 0)\n",
        "\n",
        "    if self.activation is not None:\n",
        "      return self.activation(outputs)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {'num_experts': self.num_experts}\n",
        "    base_config = super(DepthwiseCondConv2D, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "metadata": {
        "id": "tSyCLjbVlYeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# efficientnet_model"
      ],
      "metadata": {
        "id": "0OIZ1v2DQ485"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Contains definitions for EfficientNet model.\n",
        "[1] Mingxing Tan, Quoc V. Le\n",
        "  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n",
        "  ICML'19, https://arxiv.org/abs/1905.11946\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import functools\n",
        "import math\n",
        "\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import six\n",
        "from six.moves import xrange\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#import utils\n",
        "#from condconv import condconv_layers\n",
        "\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'data_format',\n",
        "    'num_classes', 'width_coefficient', 'depth_coefficient', 'depth_divisor',\n",
        "    'min_depth', 'survival_prob', 'relu_fn', 'batch_norm', 'use_se',\n",
        "    'se_coefficient', 'local_pooling', 'condconv_num_experts',\n",
        "    'clip_projection_output', 'blocks_args', 'fix_head_stem', 'use_bfloat16'\n",
        "])\n",
        "# Note: the default value of None is not necessarily valid. It is valid to leave\n",
        "# width_coefficient, depth_coefficient at None, which is treated as 1.0 (and\n",
        "# which also allows depth_divisor and min_depth to be left at None).\n",
        "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
        "\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
        "    'expand_ratio', 'id_skip', 'strides', 'se_ratio', 'conv_type', 'fused_conv',\n",
        "    'space2depth', 'condconv', 'activation_fn'\n",
        "])\n",
        "# defaults will be a public argument for namedtuple in Python 3.7\n",
        "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
        "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
        "\n",
        "\n",
        "def conv_kernel_initializer(shape, dtype=None, partition_info=None):\n",
        "  \"\"\"Initialization for convolutional kernels.\n",
        "  The main difference with tf.variance_scaling_initializer is that\n",
        "  tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n",
        "  standard deviation, whereas here we use a normal distribution. Similarly,\n",
        "  tf.initializers.variance_scaling uses a truncated normal with\n",
        "  a corrected standard deviation.\n",
        "  Args:\n",
        "    shape: shape of variable\n",
        "    dtype: dtype of variable\n",
        "    partition_info: unused\n",
        "  Returns:\n",
        "    an initialization for the variable\n",
        "  \"\"\"\n",
        "  del partition_info\n",
        "  kernel_height, kernel_width, _, out_filters = shape\n",
        "  fan_out = int(kernel_height * kernel_width * out_filters)\n",
        "  return tf.random_normal(\n",
        "      shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n",
        "\n",
        "\n",
        "def dense_kernel_initializer(shape, dtype=None, partition_info=None):\n",
        "  \"\"\"Initialization for dense kernels.\n",
        "  This initialization is equal to\n",
        "    tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',\n",
        "                                    distribution='uniform').\n",
        "  It is written out explicitly here for clarity.\n",
        "  Args:\n",
        "    shape: shape of variable\n",
        "    dtype: dtype of variable\n",
        "    partition_info: unused\n",
        "  Returns:\n",
        "    an initialization for the variable\n",
        "  \"\"\"\n",
        "  del partition_info\n",
        "  init_range = 1.0 / np.sqrt(shape[1])\n",
        "  return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n",
        "\n",
        "\n",
        "def round_filters(filters, global_params, skip=False):\n",
        "  \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
        "  orig_f = filters\n",
        "  multiplier = global_params.width_coefficient\n",
        "  divisor = global_params.depth_divisor\n",
        "  min_depth = global_params.min_depth\n",
        "  if skip or not multiplier:\n",
        "    return filters\n",
        "\n",
        "  filters *= multiplier\n",
        "  min_depth = min_depth or divisor\n",
        "  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_filters < 0.9 * filters:\n",
        "    new_filters += divisor\n",
        "  logging.info('round_filter input=%s output=%s', orig_f, new_filters)\n",
        "  return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats, global_params, skip=False):\n",
        "  \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
        "  multiplier = global_params.depth_coefficient\n",
        "  if skip or not multiplier:\n",
        "    return repeats\n",
        "  return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "\n",
        "class MBConvBlock(tf.keras.layers.Layer):\n",
        "  \"\"\"A class of MBConv: Mobile Inverted Residual Bottleneck.\n",
        "  Attributes:\n",
        "    endpoints: dict. A list of internal tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, block_args, global_params):\n",
        "    \"\"\"Initializes a MBConv block.\n",
        "    Args:\n",
        "      block_args: BlockArgs, arguments to create a Block.\n",
        "      global_params: GlobalParams, a set of global parameters.\n",
        "    \"\"\"\n",
        "    super(MBConvBlock, self).__init__()\n",
        "    self._block_args = block_args\n",
        "    self._local_pooling = global_params.local_pooling\n",
        "    self._batch_norm_momentum = global_params.batch_norm_momentum\n",
        "    self._batch_norm_epsilon = global_params.batch_norm_epsilon\n",
        "    self._batch_norm = global_params.batch_norm\n",
        "    self._condconv_num_experts = global_params.condconv_num_experts\n",
        "    self._data_format = global_params.data_format\n",
        "    self._se_coefficient = global_params.se_coefficient\n",
        "    if self._data_format == 'channels_first':\n",
        "      self._channel_axis = 1\n",
        "      self._spatial_dims = [2, 3]\n",
        "    else:\n",
        "      self._channel_axis = -1\n",
        "      self._spatial_dims = [1, 2]\n",
        "\n",
        "    self._relu_fn = (self._block_args.activation_fn\n",
        "                     or global_params.relu_fn or tf.nn.swish)\n",
        "    self._has_se = (\n",
        "        global_params.use_se and self._block_args.se_ratio is not None and\n",
        "        0 < self._block_args.se_ratio <= 1)\n",
        "\n",
        "    self._clip_projection_output = global_params.clip_projection_output\n",
        "\n",
        "    self.endpoints = None\n",
        "\n",
        "    self.conv_cls = utils.Conv2D\n",
        "    self.depthwise_conv_cls = utils.DepthwiseConv2D\n",
        "    if self._block_args.condconv:\n",
        "      self.conv_cls = functools.partial(\n",
        "          condconv_layers.CondConv2D, num_experts=self._condconv_num_experts)\n",
        "      self.depthwise_conv_cls = functools.partial(\n",
        "          condconv_layers.DepthwiseCondConv2D,\n",
        "          num_experts=self._condconv_num_experts)\n",
        "\n",
        "    # Builds the block accordings to arguments.\n",
        "    self._build()\n",
        "\n",
        "  def block_args(self):\n",
        "    return self._block_args\n",
        "\n",
        "  def _build(self):\n",
        "    \"\"\"Builds block according to the arguments.\"\"\"\n",
        "    if self._block_args.space2depth == 1:\n",
        "      self._space2depth = tf.layers.Conv2D(\n",
        "          self._block_args.input_filters,\n",
        "          kernel_size=[2, 2],\n",
        "          strides=[2, 2],\n",
        "          kernel_initializer=conv_kernel_initializer,\n",
        "          padding='same',\n",
        "          data_format=self._data_format,\n",
        "          use_bias=False)\n",
        "      self._bnsp = self._batch_norm(\n",
        "          axis=self._channel_axis,\n",
        "          momentum=self._batch_norm_momentum,\n",
        "          epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "    if self._block_args.condconv:\n",
        "      # Add the example-dependent routing function\n",
        "      self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n",
        "          data_format=self._data_format)\n",
        "      self._routing_fn = tf.layers.Dense(\n",
        "          self._condconv_num_experts, activation=tf.nn.sigmoid)\n",
        "\n",
        "    filters = self._block_args.input_filters * self._block_args.expand_ratio\n",
        "    kernel_size = self._block_args.kernel_size\n",
        "\n",
        "    # Fused expansion phase. Called if using fused convolutions.\n",
        "    self._fused_conv = self.conv_cls(\n",
        "        filters=filters,\n",
        "        kernel_size=[kernel_size, kernel_size],\n",
        "        strides=self._block_args.strides,\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._data_format,\n",
        "        use_bias=False)\n",
        "\n",
        "    # Expansion phase. Called if not using fused convolutions and expansion\n",
        "    # phase is necessary.\n",
        "    self._expand_conv = self.conv_cls(\n",
        "        filters=filters,\n",
        "        kernel_size=[1, 1],\n",
        "        strides=[1, 1],\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._data_format,\n",
        "        use_bias=False)\n",
        "    self._bn0 = self._batch_norm(\n",
        "        axis=self._channel_axis,\n",
        "        momentum=self._batch_norm_momentum,\n",
        "        epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "    # Depth-wise convolution phase. Called if not using fused convolutions.\n",
        "    self._depthwise_conv = self.depthwise_conv_cls(\n",
        "        kernel_size=[kernel_size, kernel_size],\n",
        "        strides=self._block_args.strides,\n",
        "        depthwise_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._data_format,\n",
        "        use_bias=False)\n",
        "\n",
        "    self._bn1 = self._batch_norm(\n",
        "        axis=self._channel_axis,\n",
        "        momentum=self._batch_norm_momentum,\n",
        "        epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "    if self._has_se:\n",
        "      num_reduced_filters = int(self._block_args.input_filters * (\n",
        "          self._block_args.se_ratio * (self._se_coefficient\n",
        "                                       if self._se_coefficient else 1)))\n",
        "      # account for space2depth transformation in SE filter depth, since\n",
        "      # the SE compression ratio is w.r.t. the original filter depth before\n",
        "      # space2depth is applied.\n",
        "      num_reduced_filters = (num_reduced_filters // 4\n",
        "                             if self._block_args.space2depth == 1\n",
        "                             else num_reduced_filters)\n",
        "      num_reduced_filters = max(1, num_reduced_filters)\n",
        "      # Squeeze and Excitation layer.\n",
        "      self._se_reduce = utils.Conv2D(\n",
        "          num_reduced_filters,\n",
        "          kernel_size=[1, 1],\n",
        "          strides=[1, 1],\n",
        "          kernel_initializer=conv_kernel_initializer,\n",
        "          padding='same',\n",
        "          data_format=self._data_format,\n",
        "          use_bias=True)\n",
        "      self._se_expand = utils.Conv2D(\n",
        "          filters,\n",
        "          kernel_size=[1, 1],\n",
        "          strides=[1, 1],\n",
        "          kernel_initializer=conv_kernel_initializer,\n",
        "          padding='same',\n",
        "          data_format=self._data_format,\n",
        "          use_bias=True)\n",
        "\n",
        "    # Output phase.\n",
        "    filters = self._block_args.output_filters\n",
        "    self._project_conv = self.conv_cls(\n",
        "        filters=filters,\n",
        "        kernel_size=[1, 1],\n",
        "        strides=[1, 1],\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._data_format,\n",
        "        use_bias=False)\n",
        "    self._bn2 = self._batch_norm(\n",
        "        axis=self._channel_axis,\n",
        "        momentum=self._batch_norm_momentum,\n",
        "        epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "  def _call_se(self, input_tensor):\n",
        "    \"\"\"Call Squeeze and Excitation layer.\n",
        "    Args:\n",
        "      input_tensor: Tensor, a single input tensor for Squeeze/Excitation layer.\n",
        "    Returns:\n",
        "      A output tensor, which should have the same shape as input.\n",
        "    \"\"\"\n",
        "    if self._local_pooling:\n",
        "      shape = input_tensor.get_shape().as_list()\n",
        "      kernel_size = [\n",
        "          1, shape[self._spatial_dims[0]], shape[self._spatial_dims[1]], 1]\n",
        "      se_tensor = tf.nn.avg_pool(\n",
        "          input_tensor,\n",
        "          ksize=kernel_size,\n",
        "          strides=[1, 1, 1, 1],\n",
        "          padding='VALID',\n",
        "          data_format=self._data_format)\n",
        "    else:\n",
        "      se_tensor = tf.reduce_mean(\n",
        "          input_tensor, self._spatial_dims, keepdims=True)\n",
        "    se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))\n",
        "    logging.info('Built SE %s : %s', self.name, se_tensor.shape)\n",
        "    return tf.sigmoid(se_tensor) * input_tensor\n",
        "\n",
        "  def call(self, inputs, training=True, survival_prob=None):\n",
        "    \"\"\"Implementation of call().\n",
        "    Args:\n",
        "      inputs: the inputs tensor.\n",
        "      training: boolean, whether the model is constructed for training.\n",
        "      survival_prob: float, between 0 to 1, drop connect rate.\n",
        "    Returns:\n",
        "      A output tensor.\n",
        "    \"\"\"\n",
        "    logging.info('Block %s input shape: %s', self.name, inputs.shape)\n",
        "    x = inputs\n",
        "\n",
        "    fused_conv_fn = self._fused_conv\n",
        "    expand_conv_fn = self._expand_conv\n",
        "    depthwise_conv_fn = self._depthwise_conv\n",
        "    project_conv_fn = self._project_conv\n",
        "\n",
        "    if self._block_args.condconv:\n",
        "      pooled_inputs = self._avg_pooling(inputs)\n",
        "      routing_weights = self._routing_fn(pooled_inputs)\n",
        "      # Capture routing weights as additional input to CondConv layers\n",
        "      fused_conv_fn = functools.partial(\n",
        "          self._fused_conv, routing_weights=routing_weights)\n",
        "      expand_conv_fn = functools.partial(\n",
        "          self._expand_conv, routing_weights=routing_weights)\n",
        "      depthwise_conv_fn = functools.partial(\n",
        "          self._depthwise_conv, routing_weights=routing_weights)\n",
        "      project_conv_fn = functools.partial(\n",
        "          self._project_conv, routing_weights=routing_weights)\n",
        "\n",
        "    # creates conv 2x2 kernel\n",
        "    if self._block_args.space2depth == 1:\n",
        "      with tf.variable_scope('space2depth'):\n",
        "        x = self._relu_fn(\n",
        "            self._bnsp(self._space2depth(x), training=training))\n",
        "      logging.info('Block start with space2depth shape: %s', x.shape)\n",
        "\n",
        "    if self._block_args.fused_conv:\n",
        "      # If use fused mbconv, skip expansion and use regular conv.\n",
        "      x = self._relu_fn(self._bn1(fused_conv_fn(x), training=training))\n",
        "      logging.info('Conv2D shape: %s', x.shape)\n",
        "    else:\n",
        "      # Otherwise, first apply expansion and then apply depthwise conv.\n",
        "      if self._block_args.expand_ratio != 1:\n",
        "        x = self._relu_fn(self._bn0(expand_conv_fn(x), training=training))\n",
        "        logging.info('Expand shape: %s', x.shape)\n",
        "\n",
        "      x = self._relu_fn(self._bn1(depthwise_conv_fn(x), training=training))\n",
        "      logging.info('DWConv shape: %s', x.shape)\n",
        "\n",
        "    if self._has_se:\n",
        "      with tf.variable_scope('se'):\n",
        "        x = self._call_se(x)\n",
        "\n",
        "    self.endpoints = {'expansion_output': x}\n",
        "\n",
        "    x = self._bn2(project_conv_fn(x), training=training)\n",
        "    # Add identity so that quantization-aware training can insert quantization\n",
        "    # ops correctly.\n",
        "    x = tf.identity(x)\n",
        "    if self._clip_projection_output:\n",
        "      x = tf.clip_by_value(x, -6, 6)\n",
        "    if self._block_args.id_skip:\n",
        "      if all(\n",
        "          s == 1 for s in self._block_args.strides\n",
        "      ) and inputs.get_shape().as_list()[-1] == x.get_shape().as_list()[-1]:\n",
        "        # Apply only if skip connection presents.\n",
        "        if survival_prob:\n",
        "          x = utils.drop_connect(x, training, survival_prob)\n",
        "        x = tf.add(x, inputs)\n",
        "    logging.info('Project shape: %s', x.shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "class MBConvBlockWithoutDepthwise(MBConvBlock):\n",
        "  \"\"\"MBConv-like block without depthwise convolution and squeeze-and-excite.\"\"\"\n",
        "\n",
        "  def _build(self):\n",
        "    \"\"\"Builds block according to the arguments.\"\"\"\n",
        "    filters = self._block_args.input_filters * self._block_args.expand_ratio\n",
        "    if self._block_args.expand_ratio != 1:\n",
        "      # Expansion phase:\n",
        "      self._expand_conv = tf.layers.Conv2D(\n",
        "          filters,\n",
        "          kernel_size=[3, 3],\n",
        "          strides=self._block_args.strides,\n",
        "          kernel_initializer=conv_kernel_initializer,\n",
        "          padding='same',\n",
        "          use_bias=False)\n",
        "      self._bn0 = self._batch_norm(\n",
        "          axis=self._channel_axis,\n",
        "          momentum=self._batch_norm_momentum,\n",
        "          epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "    # Output phase:\n",
        "    filters = self._block_args.output_filters\n",
        "    self._project_conv = tf.layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size=[1, 1],\n",
        "        strides=[1, 1],\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        use_bias=False)\n",
        "    self._bn1 = self._batch_norm(\n",
        "        axis=self._channel_axis,\n",
        "        momentum=self._batch_norm_momentum,\n",
        "        epsilon=self._batch_norm_epsilon)\n",
        "\n",
        "  def call(self, inputs, training=True, survival_prob=None):\n",
        "    \"\"\"Implementation of call().\n",
        "    Args:\n",
        "      inputs: the inputs tensor.\n",
        "      training: boolean, whether the model is constructed for training.\n",
        "      survival_prob: float, between 0 to 1, drop connect rate.\n",
        "    Returns:\n",
        "      A output tensor.\n",
        "    \"\"\"\n",
        "    logging.info('Block input shape: %s', inputs.shape)\n",
        "    if self._block_args.expand_ratio != 1:\n",
        "      x = self._relu_fn(self._bn0(self._expand_conv(inputs), training=training))\n",
        "    else:\n",
        "      x = inputs\n",
        "    logging.info('Expand shape: %s', x.shape)\n",
        "\n",
        "    self.endpoints = {'expansion_output': x}\n",
        "\n",
        "    x = self._bn1(self._project_conv(x), training=training)\n",
        "    # Add identity so that quantization-aware training can insert quantization\n",
        "    # ops correctly.\n",
        "    x = tf.identity(x)\n",
        "    if self._clip_projection_output:\n",
        "      x = tf.clip_by_value(x, -6, 6)\n",
        "\n",
        "    if self._block_args.id_skip:\n",
        "      if all(\n",
        "          s == 1 for s in self._block_args.strides\n",
        "      ) and self._block_args.input_filters == self._block_args.output_filters:\n",
        "        # Apply only if skip connection presents.\n",
        "        if survival_prob:\n",
        "          x = utils.drop_connect(x, training, survival_prob)\n",
        "        x = tf.add(x, inputs)\n",
        "    logging.info('Project shape: %s', x.shape)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  \"\"\"A class implements tf.keras.Model for MNAS-like model.\n",
        "    Reference: https://arxiv.org/abs/1807.11626\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, blocks_args=None, global_params=None):\n",
        "    \"\"\"Initializes an `Model` instance.\n",
        "    Args:\n",
        "      blocks_args: A list of BlockArgs to construct block modules.\n",
        "      global_params: GlobalParams, a set of global parameters.\n",
        "    Raises:\n",
        "      ValueError: when blocks_args is not specified as a list.\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    if not isinstance(blocks_args, list):\n",
        "      raise ValueError('blocks_args should be a list.')\n",
        "    self._global_params = global_params\n",
        "    self._blocks_args = blocks_args\n",
        "    self._relu_fn = global_params.relu_fn or tf.nn.swish\n",
        "    self._batch_norm = global_params.batch_norm\n",
        "    self._fix_head_stem = global_params.fix_head_stem\n",
        "\n",
        "    self.endpoints = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def _get_conv_block(self, conv_type):\n",
        "    conv_block_map = {0: MBConvBlock, 1: MBConvBlockWithoutDepthwise}\n",
        "    return conv_block_map[conv_type]\n",
        "\n",
        "  def _build(self):\n",
        "    \"\"\"Builds a model.\"\"\"\n",
        "    self._blocks = []\n",
        "    batch_norm_momentum = self._global_params.batch_norm_momentum\n",
        "    batch_norm_epsilon = self._global_params.batch_norm_epsilon\n",
        "    if self._global_params.data_format == 'channels_first':\n",
        "      channel_axis = 1\n",
        "      self._spatial_dims = [2, 3]\n",
        "    else:\n",
        "      channel_axis = -1\n",
        "      self._spatial_dims = [1, 2]\n",
        "\n",
        "    # Stem part.\n",
        "    self._conv_stem = utils.Conv2D(\n",
        "        filters=round_filters(32, self._global_params, self._fix_head_stem),\n",
        "        kernel_size=[3, 3],\n",
        "        strides=[2, 2],\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._global_params.data_format,\n",
        "        use_bias=False)\n",
        "    self._bn0 = self._batch_norm(\n",
        "        axis=channel_axis,\n",
        "        momentum=batch_norm_momentum,\n",
        "        epsilon=batch_norm_epsilon)\n",
        "\n",
        "    # Builds blocks.\n",
        "    for i, block_args in enumerate(self._blocks_args):\n",
        "      assert block_args.num_repeat > 0\n",
        "      assert block_args.space2depth in [0, 1, 2]\n",
        "      # Update block input and output filters based on depth multiplier.\n",
        "      input_filters = round_filters(block_args.input_filters,\n",
        "                                    self._global_params)\n",
        "\n",
        "      output_filters = round_filters(block_args.output_filters,\n",
        "                                     self._global_params)\n",
        "      kernel_size = block_args.kernel_size\n",
        "      if self._fix_head_stem and (i == 0 or i == len(self._blocks_args) - 1):\n",
        "        repeats = block_args.num_repeat\n",
        "      else:\n",
        "        repeats = round_repeats(block_args.num_repeat, self._global_params)\n",
        "      block_args = block_args._replace(\n",
        "          input_filters=input_filters,\n",
        "          output_filters=output_filters,\n",
        "          num_repeat=repeats)\n",
        "\n",
        "      # The first block needs to take care of stride and filter size increase.\n",
        "      conv_block = self._get_conv_block(block_args.conv_type)\n",
        "      if not block_args.space2depth:  #  no space2depth at all\n",
        "        self._blocks.append(conv_block(block_args, self._global_params))\n",
        "      else:\n",
        "        # if space2depth, adjust filters, kernels, and strides.\n",
        "        depth_factor = int(4 / block_args.strides[0] / block_args.strides[1])\n",
        "        block_args = block_args._replace(\n",
        "            input_filters=block_args.input_filters * depth_factor,\n",
        "            output_filters=block_args.output_filters * depth_factor,\n",
        "            kernel_size=((block_args.kernel_size + 1) // 2 if depth_factor > 1\n",
        "                         else block_args.kernel_size))\n",
        "        # if the first block has stride-2 and space2depth transformation\n",
        "        if (block_args.strides[0] == 2 and block_args.strides[1] == 2):\n",
        "          block_args = block_args._replace(strides=[1, 1])\n",
        "          self._blocks.append(conv_block(block_args, self._global_params))\n",
        "          block_args = block_args._replace(  # sp stops at stride-2\n",
        "              space2depth=0,\n",
        "              input_filters=input_filters,\n",
        "              output_filters=output_filters,\n",
        "              kernel_size=kernel_size)\n",
        "        elif block_args.space2depth == 1:\n",
        "          self._blocks.append(conv_block(block_args, self._global_params))\n",
        "          block_args = block_args._replace(space2depth=2)\n",
        "        else:\n",
        "          self._blocks.append(conv_block(block_args, self._global_params))\n",
        "      if block_args.num_repeat > 1:  # rest of blocks with the same block_arg\n",
        "        # pylint: disable=protected-access\n",
        "        block_args = block_args._replace(\n",
        "            input_filters=block_args.output_filters, strides=[1, 1])\n",
        "        # pylint: enable=protected-access\n",
        "      for _ in xrange(block_args.num_repeat - 1):\n",
        "        self._blocks.append(conv_block(block_args, self._global_params))\n",
        "\n",
        "    # Head part.\n",
        "    self._conv_head = utils.Conv2D(\n",
        "        filters=round_filters(1280, self._global_params, self._fix_head_stem),\n",
        "        kernel_size=[1, 1],\n",
        "        strides=[1, 1],\n",
        "        kernel_initializer=conv_kernel_initializer,\n",
        "        padding='same',\n",
        "        data_format=self._global_params.data_format,\n",
        "        use_bias=False)\n",
        "    self._bn1 = self._batch_norm(\n",
        "        axis=channel_axis,\n",
        "        momentum=batch_norm_momentum,\n",
        "        epsilon=batch_norm_epsilon)\n",
        "\n",
        "    self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n",
        "        data_format=self._global_params.data_format)\n",
        "    if self._global_params.num_classes:\n",
        "      self._fc = tf.layers.Dense(\n",
        "          self._global_params.num_classes,\n",
        "          kernel_initializer=dense_kernel_initializer)\n",
        "    else:\n",
        "      self._fc = None\n",
        "\n",
        "    if self._global_params.dropout_rate > 0:\n",
        "      self._dropout = tf.keras.layers.Dropout(self._global_params.dropout_rate)\n",
        "    else:\n",
        "      self._dropout = None\n",
        "\n",
        "  def call(self,\n",
        "           inputs,\n",
        "           training=True,\n",
        "           features_only=None,\n",
        "           pooled_features_only=False):\n",
        "    \"\"\"Implementation of call().\n",
        "    Args:\n",
        "      inputs: input tensors.\n",
        "      training: boolean, whether the model is constructed for training.\n",
        "      features_only: build the base feature network only.\n",
        "      pooled_features_only: build the base network for features extraction\n",
        "        (after 1x1 conv layer and global pooling, but before dropout and fc\n",
        "        head).\n",
        "    Returns:\n",
        "      output tensors.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    self.endpoints = {}\n",
        "    reduction_idx = 0\n",
        "    # Calls Stem layers\n",
        "    with tf.variable_scope('stem'):\n",
        "      outputs = self._relu_fn(\n",
        "          self._bn0(self._conv_stem(inputs), training=training))\n",
        "    logging.info('Built stem layers with output shape: %s', outputs.shape)\n",
        "    self.endpoints['stem'] = outputs\n",
        "\n",
        "    # Calls blocks.\n",
        "    for idx, block in enumerate(self._blocks):\n",
        "      is_reduction = False  # reduction flag for blocks after the stem layer\n",
        "      # If the first block has space-to-depth layer, then stem is\n",
        "      # the first reduction point.\n",
        "      if (block.block_args().space2depth == 1 and idx == 0):\n",
        "        reduction_idx += 1\n",
        "        self.endpoints['reduction_%s' % reduction_idx] = outputs\n",
        "\n",
        "      elif ((idx == len(self._blocks) - 1) or\n",
        "            self._blocks[idx + 1].block_args().strides[0] > 1):\n",
        "        is_reduction = True\n",
        "        reduction_idx += 1\n",
        "\n",
        "      with tf.variable_scope('blocks_%s' % idx):\n",
        "        survival_prob = self._global_params.survival_prob\n",
        "        if survival_prob:\n",
        "          drop_rate = 1.0 - survival_prob\n",
        "          survival_prob = 1.0 - drop_rate * float(idx) / len(self._blocks)\n",
        "          logging.info('block_%s survival_prob: %s', idx, survival_prob)\n",
        "        outputs = block.call(\n",
        "            outputs, training=training, survival_prob=survival_prob)\n",
        "        self.endpoints['block_%s' % idx] = outputs\n",
        "        if is_reduction:\n",
        "          self.endpoints['reduction_%s' % reduction_idx] = outputs\n",
        "        if block.endpoints:\n",
        "          for k, v in six.iteritems(block.endpoints):\n",
        "            self.endpoints['block_%s/%s' % (idx, k)] = v\n",
        "            if is_reduction:\n",
        "              self.endpoints['reduction_%s/%s' % (reduction_idx, k)] = v\n",
        "    self.endpoints['features'] = outputs\n",
        "\n",
        "    if not features_only:\n",
        "      # Calls final layers and returns logits.\n",
        "      with tf.variable_scope('head'):\n",
        "        outputs = self._relu_fn(\n",
        "            self._bn1(self._conv_head(outputs), training=training))\n",
        "        self.endpoints['head_1x1'] = outputs\n",
        "\n",
        "        if self._global_params.local_pooling:\n",
        "          shape = outputs.get_shape().as_list()\n",
        "          kernel_size = [\n",
        "              1, shape[self._spatial_dims[0]], shape[self._spatial_dims[1]], 1]\n",
        "          outputs = tf.nn.avg_pool(\n",
        "              outputs, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')\n",
        "          self.endpoints['pooled_features'] = outputs\n",
        "          if not pooled_features_only:\n",
        "            if self._dropout:\n",
        "              outputs = self._dropout(outputs, training=training)\n",
        "            self.endpoints['global_pool'] = outputs\n",
        "            if self._fc:\n",
        "              outputs = tf.squeeze(outputs, self._spatial_dims)\n",
        "              outputs = self._fc(outputs)\n",
        "            self.endpoints['head'] = outputs\n",
        "        else:\n",
        "          outputs = self._avg_pooling(outputs)\n",
        "          self.endpoints['pooled_features'] = outputs\n",
        "          if not pooled_features_only:\n",
        "            if self._dropout:\n",
        "              outputs = self._dropout(outputs, training=training)\n",
        "            self.endpoints['global_pool'] = outputs\n",
        "            if self._fc:\n",
        "              outputs = self._fc(outputs)\n",
        "            self.endpoints['head'] = outputs\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "WaOLcxlgkcgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# efficientnet_builder"
      ],
      "metadata": {
        "id": "1sObeD81Q9Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Model Builder for EfficientNet.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import re\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#import efficientnet_model\n",
        "#import utils\n",
        "MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
        "STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "  \"\"\"Get efficientnet params based on model name.\"\"\"\n",
        "  params_dict = {\n",
        "      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n",
        "      'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "      'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "      'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "      'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "      'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "      'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "      'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "      'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "      'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
        "      'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
        "  }\n",
        "  return params_dict[model_name]\n",
        "\n",
        "\n",
        "class BlockDecoder(object):\n",
        "  \"\"\"Block Decoder for readability.\"\"\"\n",
        "\n",
        "  def _decode_block_string(self, block_string):\n",
        "    \"\"\"Gets a block through a string notation of arguments.\"\"\"\n",
        "    if six.PY2:\n",
        "      assert isinstance(block_string, (str, unicode))\n",
        "    else:\n",
        "      assert isinstance(block_string, str)\n",
        "    ops = block_string.split('_')\n",
        "    options = {}\n",
        "    for op in ops:\n",
        "      splits = re.split(r'(\\d.*)', op)\n",
        "      if len(splits) >= 2:\n",
        "        key, value = splits[:2]\n",
        "        options[key] = value\n",
        "\n",
        "    if 's' not in options or len(options['s']) != 2:\n",
        "      raise ValueError('Strides options should be a pair of integers.')\n",
        "\n",
        "    return efficientnet_model.BlockArgs(\n",
        "        kernel_size=int(options['k']),\n",
        "        num_repeat=int(options['r']),\n",
        "        input_filters=int(options['i']),\n",
        "        output_filters=int(options['o']),\n",
        "        expand_ratio=int(options['e']),\n",
        "        id_skip=('noskip' not in block_string),\n",
        "        se_ratio=float(options['se']) if 'se' in options else None,\n",
        "        strides=[int(options['s'][0]),\n",
        "                 int(options['s'][1])],\n",
        "        conv_type=int(options['c']) if 'c' in options else 0,\n",
        "        fused_conv=int(options['f']) if 'f' in options else 0,\n",
        "        space2depth=int(options['d']) if 'd' in options else 0,\n",
        "        condconv=('cc' in block_string),\n",
        "        activation_fn=(tf.nn.relu if int(options['a']) == 0\n",
        "                       else tf.nn.swish) if 'a' in options else None)\n",
        "\n",
        "  def _encode_block_string(self, block):\n",
        "    \"\"\"Encodes a block to a string.\"\"\"\n",
        "    args = [\n",
        "        'r%d' % block.num_repeat,\n",
        "        'k%d' % block.kernel_size,\n",
        "        's%d%d' % (block.strides[0], block.strides[1]),\n",
        "        'e%s' % block.expand_ratio,\n",
        "        'i%d' % block.input_filters,\n",
        "        'o%d' % block.output_filters,\n",
        "        'c%d' % block.conv_type,\n",
        "        'f%d' % block.fused_conv,\n",
        "        'd%d' % block.space2depth,\n",
        "    ]\n",
        "    if block.se_ratio > 0 and block.se_ratio <= 1:\n",
        "      args.append('se%s' % block.se_ratio)\n",
        "    if block.id_skip is False:  # pylint: disable=g-bool-id-comparison\n",
        "      args.append('noskip')\n",
        "    if block.condconv:\n",
        "      args.append('cc')\n",
        "    return '_'.join(args)\n",
        "\n",
        "  def decode(self, string_list):\n",
        "    \"\"\"Decodes a list of string notations to specify blocks inside the network.\n",
        "    Args:\n",
        "      string_list: a list of strings, each string is a notation of block.\n",
        "    Returns:\n",
        "      A list of namedtuples to represent blocks arguments.\n",
        "    \"\"\"\n",
        "    assert isinstance(string_list, list)\n",
        "    blocks_args = []\n",
        "    for block_string in string_list:\n",
        "      blocks_args.append(self._decode_block_string(block_string))\n",
        "    return blocks_args\n",
        "\n",
        "  def encode(self, blocks_args):\n",
        "    \"\"\"Encodes a list of Blocks to a list of strings.\n",
        "    Args:\n",
        "      blocks_args: A list of namedtuples to represent blocks arguments.\n",
        "    Returns:\n",
        "      a list of strings, each string is a notation of block.\n",
        "    \"\"\"\n",
        "    block_strings = []\n",
        "    for block in blocks_args:\n",
        "      block_strings.append(self._encode_block_string(block))\n",
        "    return block_strings\n",
        "\n",
        "\n",
        "def swish(features, use_native=True, use_hard=False):\n",
        "  \"\"\"Computes the Swish activation function.\n",
        "  We provide three alternnatives:\n",
        "    - Native tf.nn.swish, use less memory during training than composable swish.\n",
        "    - Quantization friendly hard swish.\n",
        "    - A composable swish, equivalant to tf.nn.swish, but more general for\n",
        "      finetuning and TF-Hub.\n",
        "  Args:\n",
        "    features: A `Tensor` representing preactivation values.\n",
        "    use_native: Whether to use the native swish from tf.nn that uses a custom\n",
        "      gradient to reduce memory usage, or to use customized swish that uses\n",
        "      default TensorFlow gradient computation.\n",
        "    use_hard: Whether to use quantization-friendly hard swish.\n",
        "  Returns:\n",
        "    The activation value.\n",
        "  \"\"\"\n",
        "  if use_native and use_hard:\n",
        "    raise ValueError('Cannot specify both use_native and use_hard.')\n",
        "\n",
        "  if use_native:\n",
        "    return tf.nn.swish(features)\n",
        "\n",
        "  if use_hard:\n",
        "    return features * tf.nn.relu6(features + np.float32(3)) * (1. / 6.)\n",
        "\n",
        "  features = tf.convert_to_tensor(features, name='features')\n",
        "  return features * tf.nn.sigmoid(features)\n",
        "\n",
        "\n",
        "_DEFAULT_BLOCKS_ARGS = [\n",
        "    'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "    'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "    'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "    'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "]\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None,\n",
        "                 depth_coefficient=None,\n",
        "                 dropout_rate=0.2,\n",
        "                 survival_prob=0.8):\n",
        "  \"\"\"Creates a efficientnet model.\"\"\"\n",
        "  global_params = efficientnet_model.GlobalParams(\n",
        "      blocks_args=_DEFAULT_BLOCKS_ARGS,\n",
        "      batch_norm_momentum=0.99,\n",
        "      batch_norm_epsilon=1e-3,\n",
        "      dropout_rate=dropout_rate,\n",
        "      survival_prob=survival_prob,\n",
        "      data_format='channels_last',\n",
        "      num_classes=1000,\n",
        "      width_coefficient=width_coefficient,\n",
        "      depth_coefficient=depth_coefficient,\n",
        "      depth_divisor=8,\n",
        "      min_depth=None,\n",
        "      relu_fn=tf.nn.swish,\n",
        "      # The default is TPU-specific batch norm.\n",
        "      # The alternative is tf.layers.BatchNormalization.\n",
        "      batch_norm=utils.train_batch_norm,  # TPU-specific requirement.\n",
        "      use_se=True,\n",
        "      clip_projection_output=False)\n",
        "  return global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "  \"\"\"Get the block args and global params for a given model.\"\"\"\n",
        "  if model_name.startswith('efficientnet'):\n",
        "    width_coefficient, depth_coefficient, _, dropout_rate = (\n",
        "        efficientnet_params(model_name))\n",
        "    global_params = efficientnet(\n",
        "        width_coefficient, depth_coefficient, dropout_rate)\n",
        "  else:\n",
        "    raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "\n",
        "  if override_params:\n",
        "    # ValueError will be raised here if override_params has fields not included\n",
        "    # in global_params.\n",
        "    global_params = global_params._replace(**override_params)\n",
        "\n",
        "  decoder = BlockDecoder()\n",
        "  blocks_args = decoder.decode(global_params.blocks_args)\n",
        "\n",
        "  logging.info('global_params= %s', global_params)\n",
        "  return blocks_args, global_params\n",
        "\n",
        "\n",
        "def build_model(images,\n",
        "                model_name,\n",
        "                training,\n",
        "                override_params=None,\n",
        "                model_dir=None,\n",
        "                fine_tuning=False,\n",
        "                features_only=False,\n",
        "                pooled_features_only=False):\n",
        "  \"\"\"A helper function to create a model and return predicted logits.\n",
        "  Args:\n",
        "    images: input images tensor.\n",
        "    model_name: string, the predefined model name.\n",
        "    training: boolean, whether the model is constructed for training.\n",
        "    override_params: A dictionary of params for overriding. Fields must exist in\n",
        "      efficientnet_model.GlobalParams.\n",
        "    model_dir: string, optional model dir for saving configs.\n",
        "    fine_tuning: boolean, whether the model is used for finetuning.\n",
        "    features_only: build the base feature network only (excluding final\n",
        "      1x1 conv layer, global pooling, dropout and fc head).\n",
        "    pooled_features_only: build the base network for features extraction (after\n",
        "      1x1 conv layer and global pooling, but before dropout and fc head).\n",
        "  Returns:\n",
        "    logits: the logits tensor of classes.\n",
        "    endpoints: the endpoints for each layer.\n",
        "  Raises:\n",
        "    When model_name specified an undefined model, raises NotImplementedError.\n",
        "    When override_params has invalid fields, raises ValueError.\n",
        "  \"\"\"\n",
        "  assert isinstance(images, tf.Tensor)\n",
        "  assert not (features_only and pooled_features_only)\n",
        "\n",
        "  # For backward compatibility.\n",
        "  if override_params and override_params.get('drop_connect_rate', None):\n",
        "    override_params['survival_prob'] = 1 - override_params['drop_connect_rate']\n",
        "\n",
        "  if not training or fine_tuning:\n",
        "    if not override_params:\n",
        "      override_params = {}\n",
        "    override_params['batch_norm'] = utils.eval_batch_norm\n",
        "    if fine_tuning:\n",
        "      override_params['relu_fn'] = functools.partial(swish, use_native=False)\n",
        "  blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "\n",
        "  if model_dir:\n",
        "    param_file = os.path.join(model_dir, 'model_params.txt')\n",
        "    if not tf.gfile.Exists(param_file):\n",
        "      if not tf.gfile.Exists(model_dir):\n",
        "        tf.gfile.MakeDirs(model_dir)\n",
        "      with tf.gfile.GFile(param_file, 'w') as f:\n",
        "        logging.info('writing to %s', param_file)\n",
        "        f.write('model_name= %s\\n\\n' % model_name)\n",
        "        f.write('global_params= %s\\n\\n' % str(global_params))\n",
        "        f.write('blocks_args= %s\\n\\n' % str(blocks_args))\n",
        "\n",
        "  with tf.variable_scope(model_name):\n",
        "    model = efficientnet_model.Model(blocks_args, global_params)\n",
        "    outputs = model(\n",
        "        images,\n",
        "        training=training,\n",
        "        features_only=features_only,\n",
        "        pooled_features_only=pooled_features_only)\n",
        "  if features_only:\n",
        "    outputs = tf.identity(outputs, 'features')\n",
        "  elif pooled_features_only:\n",
        "    outputs = tf.identity(outputs, 'pooled_features')\n",
        "  else:\n",
        "    outputs = tf.identity(outputs, 'logits')\n",
        "  return outputs, model.endpoints\n",
        "\n",
        "\n",
        "def build_model_base(images, model_name, training, override_params=None):\n",
        "  \"\"\"Create a base feature network and return the features before pooling.\n",
        "  Args:\n",
        "    images: input images tensor.\n",
        "    model_name: string, the predefined model name.\n",
        "    training: boolean, whether the model is constructed for training.\n",
        "    override_params: A dictionary of params for overriding. Fields must exist in\n",
        "      efficientnet_model.GlobalParams.\n",
        "  Returns:\n",
        "    features: base features before pooling.\n",
        "    endpoints: the endpoints for each layer.\n",
        "  Raises:\n",
        "    When model_name specified an undefined model, raises NotImplementedError.\n",
        "    When override_params has invalid fields, raises ValueError.\n",
        "  \"\"\"\n",
        "  assert isinstance(images, tf.Tensor)\n",
        "  # For backward compatibility.\n",
        "  if override_params and override_params.get('drop_connect_rate', None):\n",
        "    override_params['survival_prob'] = 1 - override_params['drop_connect_rate']\n",
        "\n",
        "  blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "\n",
        "  with tf.variable_scope(model_name):\n",
        "    model = efficientnet_model.Model(blocks_args, global_params)\n",
        "    features = model(images, training=training, features_only=True)\n",
        "\n",
        "  features = tf.identity(features, 'features')\n",
        "  return features, model.endpoints"
      ],
      "metadata": {
        "id": "8LaULfwQiFf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# efficientnet_condconv_builder"
      ],
      "metadata": {
        "id": "M0SuJXQURHcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Builder for EfficientNet-CondConv models.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#import efficientnet_builder\n",
        "#import efficientnet_model\n",
        "#import utils\n",
        "\n",
        "# The input tensor is in the range of [0, 255], we need to scale them to the\n",
        "# range of [0, 1]\n",
        "MEAN_RGB = [127.0, 127.0, 127.0]\n",
        "STDDEV_RGB = [128.0, 128.0, 128.0]\n",
        "\n",
        "\n",
        "def efficientnet_condconv_params(model_name):\n",
        "  \"\"\"Get efficientnet-condconv params based on model name.\"\"\"\n",
        "  params_dict = {\n",
        "      # (width_coefficient, depth_coefficient, resolution, dropout_rate,\n",
        "      #  condconv_num_experts)\n",
        "      'efficientnet-condconv-b0-4e': (1.0, 1.0, 224, 0.25, 4),\n",
        "      'efficientnet-condconv-b0-8e': (1.0, 1.0, 224, 0.25, 8),\n",
        "      'efficientnet-condconv-b0-8e-depth': (1.0, 1.1, 224, 0.25, 8)\n",
        "  }\n",
        "  return params_dict[model_name]\n",
        "\n",
        "\n",
        "def efficientnet_condconv(width_coefficient=None,\n",
        "                          depth_coefficient=None,\n",
        "                          dropout_rate=0.2,\n",
        "                          survival_prob=0.8,\n",
        "                          condconv_num_experts=None):\n",
        "  \"\"\"Creates an efficientnet-condconv model.\"\"\"\n",
        "  blocks_args = [\n",
        "      'r1_k3_s11_e1_i32_o16_se0.25',\n",
        "      'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "      'r2_k5_s22_e6_i24_o40_se0.25',\n",
        "      'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "      'r3_k5_s11_e6_i80_o112_se0.25_cc',\n",
        "      'r4_k5_s22_e6_i112_o192_se0.25_cc',\n",
        "      'r1_k3_s11_e6_i192_o320_se0.25_cc',\n",
        "  ]\n",
        "  global_params = efficientnet_model.GlobalParams(\n",
        "      batch_norm_momentum=0.99,\n",
        "      batch_norm_epsilon=1e-3,\n",
        "      dropout_rate=dropout_rate,\n",
        "      survival_prob=survival_prob,\n",
        "      data_format='channels_last',\n",
        "      num_classes=1000,\n",
        "      width_coefficient=width_coefficient,\n",
        "      depth_coefficient=depth_coefficient,\n",
        "      depth_divisor=8,\n",
        "      min_depth=None,\n",
        "      relu_fn=tf.nn.swish,\n",
        "      # The default is TPU-specific batch norm.\n",
        "      # The alternative is tf.layers.BatchNormalization.\n",
        "      batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.\n",
        "      use_se=True,\n",
        "      condconv_num_experts=condconv_num_experts)\n",
        "  decoder = efficientnet_builder.BlockDecoder()\n",
        "  return decoder.decode(blocks_args), global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "  \"\"\"Get the block args and global params for a given model.\"\"\"\n",
        "  if model_name.startswith('efficientnet-condconv'):\n",
        "    (width_coefficient, depth_coefficient, _, dropout_rate,\n",
        "     condconv_num_experts) = (\n",
        "         efficientnet_condconv_params(model_name))\n",
        "    blocks_args, global_params = efficientnet_condconv(\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        dropout_rate=dropout_rate,\n",
        "        condconv_num_experts=condconv_num_experts)\n",
        "  else:\n",
        "    raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "\n",
        "  if override_params:\n",
        "    # ValueError will be raised here if override_params has fields not included\n",
        "    # in global_params.\n",
        "    global_params = global_params._replace(**override_params)\n",
        "\n",
        "  tf.logging.info('global_params= %s', global_params)\n",
        "  tf.logging.info('blocks_args= %s', blocks_args)\n",
        "  return blocks_args, global_params\n",
        "\n",
        "\n",
        "def build_model(images,\n",
        "                model_name,\n",
        "                training,\n",
        "                override_params=None,\n",
        "                model_dir=None,\n",
        "                fine_tuning=False):\n",
        "  \"\"\"A helper functiion to creates a model and returns predicted logits.\n",
        "  Args:\n",
        "    images: input images tensor.\n",
        "    model_name: string, the predefined model name.\n",
        "    training: boolean, whether the model is constructed for training.\n",
        "    override_params: A dictionary of params for overriding. Fields must exist in\n",
        "      efficientnet_model.GlobalParams.\n",
        "    model_dir: string, optional model dir for saving configs.\n",
        "    fine_tuning: boolean, whether the model is used for finetuning.\n",
        "  Returns:\n",
        "    logits: the logits tensor of classes.\n",
        "    endpoints: the endpoints for each layer.\n",
        "  Raises:\n",
        "    When model_name specified an undefined model, raises NotImplementedError.\n",
        "    When override_params has invalid fields, raises ValueError.\n",
        "  \"\"\"\n",
        "  assert isinstance(images, tf.Tensor)\n",
        "  if not training or fine_tuning:\n",
        "    if not override_params:\n",
        "      override_params = {}\n",
        "    override_params['batch_norm'] = utils.BatchNormalization\n",
        "  blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "  if not training or fine_tuning:\n",
        "    global_params = global_params._replace(batch_norm=utils.BatchNormalization)\n",
        "\n",
        "  if model_dir:\n",
        "    param_file = os.path.join(model_dir, 'model_params.txt')\n",
        "    if not tf.gfile.Exists(param_file):\n",
        "      if not tf.gfile.Exists(model_dir):\n",
        "        tf.gfile.MakeDirs(model_dir)\n",
        "      with tf.gfile.GFile(param_file, 'w') as f:\n",
        "        tf.logging.info('writing to %s' % param_file)\n",
        "        f.write('model_name= %s\\n\\n' % model_name)\n",
        "        f.write('global_params= %s\\n\\n' % str(global_params))\n",
        "        f.write('blocks_args= %s\\n\\n' % str(blocks_args))\n",
        "\n",
        "  with tf.variable_scope(model_name):\n",
        "    model = efficientnet_model.Model(blocks_args, global_params)\n",
        "    logits = model(images, training=training)\n",
        "\n",
        "  logits = tf.identity(logits, 'logits')\n",
        "  return logits, model.endpoints\n",
        "\n",
        "\n",
        "def build_model_base(images, model_name, training, override_params=None):\n",
        "  \"\"\"A helper functiion to create a base model and return global_pool.\n",
        "  Args:\n",
        "    images: input images tensor.\n",
        "    model_name: string, the model name of a pre-defined MnasNet.\n",
        "    training: boolean, whether the model is constructed for training.\n",
        "    override_params: A dictionary of params for overriding. Fields must exist in\n",
        "      mnasnet_model.GlobalParams.\n",
        "  Returns:\n",
        "    features: global pool features.\n",
        "    endpoints: the endpoints for each layer.\n",
        "  Raises:\n",
        "    When model_name specified an undefined model, raises NotImplementedError.\n",
        "    When override_params has invalid fields, raises ValueError.\n",
        "  \"\"\"\n",
        "  assert isinstance(images, tf.Tensor)\n",
        "  blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "\n",
        "  with tf.variable_scope(model_name):\n",
        "    model = efficientnet_model.Model(blocks_args, global_params)\n",
        "    features = model(images, training=training, features_only=True)\n",
        "\n",
        "  features = tf.identity(features, 'global_pool')\n",
        "  return features, model.endpoints"
      ],
      "metadata": {
        "id": "WitsCIxE0OmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "GtbCtNwlRLna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"ImageNet preprocessing.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "CROP_FRACTION = 0.875\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image_bytes,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image_bytes: `Tensor` of binary image data.\n",
        "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
        "        where each coordinate is [0, 1) and the coordinates are arranged\n",
        "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
        "        image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "        area of the image must contain at least this fraction of any bounding\n",
        "        box supplied.\n",
        "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
        "        image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `float`s. The cropped area of the image\n",
        "        must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "        region of the image of the specified constraints. After `max_attempts`\n",
        "        failures, return the entire image.\n",
        "    scope: Optional `str` for name scope.\n",
        "  Returns:\n",
        "    cropped image `Tensor`\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image_bytes, bbox]):\n",
        "    shape = tf.image.extract_jpeg_shape(image_bytes)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n",
        "    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def _at_least_x_are_equal(a, b, x):\n",
        "  \"\"\"At least `x` of `a` and `b` `Tensors` are equal.\"\"\"\n",
        "  match = tf.equal(a, b)\n",
        "  match = tf.cast(match, tf.int32)\n",
        "  return tf.greater_equal(tf.reduce_sum(match), x)\n",
        "\n",
        "\n",
        "def _resize_image(image, image_size, method=None):\n",
        "  if method is not None:\n",
        "    tf.logging.info('Use customized resize method {}'.format(method))\n",
        "    return tf.image.resize([image], [image_size, image_size], method)[0]\n",
        "  tf.logging.info('Use default resize_bicubic.')\n",
        "  return tf.image.resize_bicubic([image], [image_size, image_size])[0]\n",
        "\n",
        "\n",
        "def _decode_and_random_crop(image_bytes, image_size, resize_method=None):\n",
        "  \"\"\"Make a random crop of image_size.\"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image_bytes,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4, 4. / 3.),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=10,\n",
        "      scope=None)\n",
        "  original_shape = tf.image.extract_jpeg_shape(image_bytes)\n",
        "  bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n",
        "\n",
        "  image = tf.cond(\n",
        "      bad,\n",
        "      lambda: _decode_and_center_crop(image_bytes, image_size),\n",
        "      lambda: _resize_image(image, image_size, resize_method))\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def _decode_and_center_crop(image_bytes, image_size, resize_method=None):\n",
        "  \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\n",
        "  shape = tf.image.extract_jpeg_shape(image_bytes)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "\n",
        "  # crop_fraction = image_size / (image_size + crop_padding)\n",
        "  crop_padding = round(image_size * (1/CROP_FRACTION - 1))\n",
        "  padded_center_crop_size = tf.cast(\n",
        "      ((image_size / (image_size + crop_padding)) *\n",
        "       tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n",
        "      tf.int32)\n",
        "\n",
        "  offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n",
        "  offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n",
        "  crop_window = tf.stack([offset_height, offset_width,\n",
        "                          padded_center_crop_size, padded_center_crop_size])\n",
        "  image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n",
        "  image = _resize_image(image, image_size, resize_method)\n",
        "  return image\n",
        "\n",
        "\n",
        "def _flip(image):\n",
        "  \"\"\"Random horizontal image flip.\"\"\"\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_train(image_bytes,\n",
        "                         use_bfloat16,\n",
        "                         image_size=IMAGE_SIZE,\n",
        "                         augment_name=None,\n",
        "                         randaug_num_layers=None,\n",
        "                         randaug_magnitude=None,\n",
        "                         resize_method=None):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image_bytes: `Tensor` representing an image binary of arbitrary size.\n",
        "    use_bfloat16: `bool` for whether to use bfloat16.\n",
        "    image_size: image size.\n",
        "    augment_name: `string` that is the name of the augmentation method\n",
        "      to apply to the image. `autoaugment` if AutoAugment is to be used or\n",
        "      `randaugment` if RandAugment is to be used. If the value is `None` no\n",
        "      augmentation method will be applied applied. See autoaugment.py for more\n",
        "      details.\n",
        "    randaug_num_layers: 'int', if RandAug is used, what should the number of\n",
        "      layers be. See autoaugment.py for detailed description.\n",
        "    randaug_magnitude: 'int', if RandAug is used, what should the magnitude\n",
        "      be. See autoaugment.py for detailed description.\n",
        "    resize_method: resize method. If none, use bicubic.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n",
        "  image = _flip(image)\n",
        "  image = tf.reshape(image, [image_size, image_size, 3])\n",
        "\n",
        "  if augment_name:\n",
        "    try:\n",
        "      import autoaugment  # pylint: disable=g-import-not-at-top\n",
        "    except ImportError as e:\n",
        "      logging.exception('Autoaugment is not supported in TF 2.x.')\n",
        "      raise e\n",
        "\n",
        "    logging.info('Apply AutoAugment policy %s', augment_name)\n",
        "    input_image_type = image.dtype\n",
        "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "    image = tf.cast(image, dtype=tf.uint8)\n",
        "\n",
        "    if augment_name == 'autoaugment':\n",
        "      logging.info('Apply AutoAugment policy %s', augment_name)\n",
        "      image = autoaugment.distort_image_with_autoaugment(image, 'v0')\n",
        "    elif augment_name == 'randaugment':\n",
        "      image = autoaugment.distort_image_with_randaugment(\n",
        "          image, randaug_num_layers, randaug_magnitude)\n",
        "    else:\n",
        "      raise ValueError('Invalid value for augment_name: %s' % (augment_name))\n",
        "\n",
        "    image = tf.cast(image, dtype=input_image_type)\n",
        "\n",
        "  image = tf.image.convert_image_dtype(\n",
        "      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image_bytes,\n",
        "                        use_bfloat16,\n",
        "                        image_size=IMAGE_SIZE,\n",
        "                        resize_method=None):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image_bytes: `Tensor` representing an image binary of arbitrary size.\n",
        "    use_bfloat16: `bool` for whether to use bfloat16.\n",
        "    image_size: image size.\n",
        "    resize_method: if None, use bicubic.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n",
        "  image = tf.reshape(image, [image_size, image_size, 3])\n",
        "  image = tf.image.convert_image_dtype(\n",
        "      image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image_bytes,\n",
        "                     is_training=False,\n",
        "                     use_bfloat16=False,\n",
        "                     image_size=IMAGE_SIZE,\n",
        "                     augment_name=None,\n",
        "                     randaug_num_layers=None,\n",
        "                     randaug_magnitude=None,\n",
        "                     resize_method=None):\n",
        "  \"\"\"Preprocesses the given image.\n",
        "  Args:\n",
        "    image_bytes: `Tensor` representing an image binary of arbitrary size.\n",
        "    is_training: `bool` for whether the preprocessing is for training.\n",
        "    use_bfloat16: `bool` for whether to use bfloat16.\n",
        "    image_size: image size.\n",
        "    augment_name: `string` that is the name of the augmentation method\n",
        "      to apply to the image. `autoaugment` if AutoAugment is to be used or\n",
        "      `randaugment` if RandAugment is to be used. If the value is `None` no\n",
        "      augmentation method will be applied applied. See autoaugment.py for more\n",
        "      details.\n",
        "    randaug_num_layers: 'int', if RandAug is used, what should the number of\n",
        "      layers be. See autoaugment.py for detailed description.\n",
        "    randaug_magnitude: 'int', if RandAug is used, what should the magnitude\n",
        "      be. See autoaugment.py for detailed description.\n",
        "    resize_method: 'string' or None. Use resize_bicubic in default.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor` with value range of [0, 255].\n",
        "  \"\"\"\n",
        "  if is_training:\n",
        "    return preprocess_for_train(\n",
        "        image_bytes, use_bfloat16, image_size, augment_name,\n",
        "        randaug_num_layers, randaug_magnitude, resize_method)\n",
        "  else:\n",
        "    return preprocess_for_eval(image_bytes, use_bfloat16, image_size,\n",
        "                               resize_method)"
      ],
      "metadata": {
        "id": "j36ZPdoTtr5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imagenet_input"
      ],
      "metadata": {
        "id": "Uk10Ke1wROsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Efficient ImageNet input pipeline using tf.data.Dataset.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import collections\n",
        "import functools\n",
        "import os\n",
        "\n",
        "from absl import logging\n",
        "import six\n",
        "import tensorflow.compat.v1 as tf\n",
        "from tensorflow.compat.v1 import estimator as tf_estimator\n",
        "\n",
        "#import preprocessing\n",
        "\n",
        "\n",
        "def build_image_serving_input_fn(image_size,\n",
        "                                 batch_size=None,\n",
        "                                 resize_method=None):\n",
        "  \"\"\"Builds a serving input fn for raw images.\"\"\"\n",
        "\n",
        "  def _image_serving_input_fn():\n",
        "    \"\"\"Serving input fn for raw images.\"\"\"\n",
        "\n",
        "    def _preprocess_image(image_bytes):\n",
        "      \"\"\"Preprocess a single raw image.\"\"\"\n",
        "      image = preprocessing.preprocess_image(\n",
        "          image_bytes=image_bytes,\n",
        "          is_training=False,\n",
        "          image_size=image_size,\n",
        "          resize_method=resize_method)\n",
        "      return image\n",
        "\n",
        "    image_bytes_list = tf.placeholder(\n",
        "        shape=[batch_size],\n",
        "        dtype=tf.string,\n",
        "    )\n",
        "    images = tf.map_fn(\n",
        "        _preprocess_image, image_bytes_list, back_prop=False, dtype=tf.float32)\n",
        "    return tf_estimator.export.ServingInputReceiver(\n",
        "        images, {'image_bytes': image_bytes_list})\n",
        "  return _image_serving_input_fn\n",
        "\n",
        "\n",
        "class ImageNetTFExampleInput(six.with_metaclass(abc.ABCMeta, object)):\n",
        "  \"\"\"Base class for ImageNet input_fn generator.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               is_training,\n",
        "               use_bfloat16,\n",
        "               num_cores=8,\n",
        "               image_size=224,\n",
        "               transpose_input=False,\n",
        "               num_label_classes=1000,\n",
        "               include_background_label=False,\n",
        "               augment_name=None,\n",
        "               mixup_alpha=0.0,\n",
        "               randaug_num_layers=None,\n",
        "               randaug_magnitude=None,\n",
        "               resize_method=None):\n",
        "    \"\"\"Constructor.\n",
        "    Args:\n",
        "      is_training: `bool` for whether the input is for training\n",
        "      use_bfloat16: If True, use bfloat16 precision; else use float32.\n",
        "      num_cores: `int` for the number of TPU cores\n",
        "      image_size: `int` for image size (both width and height).\n",
        "      transpose_input: 'bool' for whether to use the double transpose trick\n",
        "      num_label_classes: number of label classes. Default to 1000 for ImageNet.\n",
        "      include_background_label: If true, label #0 is reserved for background.\n",
        "      augment_name: `string` that is the name of the augmentation method to\n",
        "        apply to the image. `autoaugment` if AutoAugment is to be used or\n",
        "        `randaugment` if RandAugment is to be used. If the value is `None` no no\n",
        "        augmentation method will be applied applied. See autoaugment.py for more\n",
        "        details.\n",
        "      mixup_alpha: float to control the strength of Mixup regularization, set to\n",
        "        0.0 to disable.\n",
        "      randaug_num_layers: 'int', if RandAug is used, what should the number of\n",
        "        layers be. See autoaugment.py for detailed description.\n",
        "      randaug_magnitude: 'int', if RandAug is used, what should the magnitude\n",
        "        be. See autoaugment.py for detailed description.\n",
        "      resize_method: If None, use bicubic in default.\n",
        "    \"\"\"\n",
        "    self.image_preprocessing_fn = preprocessing.preprocess_image\n",
        "    self.is_training = is_training\n",
        "    self.use_bfloat16 = use_bfloat16\n",
        "    self.num_cores = num_cores\n",
        "    self.transpose_input = transpose_input\n",
        "    self.image_size = image_size\n",
        "    self.include_background_label = include_background_label\n",
        "    self.num_label_classes = num_label_classes\n",
        "    if include_background_label:\n",
        "      self.num_label_classes += 1\n",
        "    self.augment_name = augment_name\n",
        "    self.mixup_alpha = mixup_alpha\n",
        "    self.randaug_num_layers = randaug_num_layers\n",
        "    self.randaug_magnitude = randaug_magnitude\n",
        "    self.resize_method = resize_method\n",
        "\n",
        "  def set_shapes(self, batch_size, images, labels):\n",
        "    \"\"\"Statically set the batch_size dimension.\"\"\"\n",
        "    if self.transpose_input:\n",
        "      images.set_shape(images.get_shape().merge_with(\n",
        "          tf.TensorShape([None, None, None, batch_size])))\n",
        "      labels.set_shape(labels.get_shape().merge_with(\n",
        "          tf.TensorShape([batch_size, None])))\n",
        "      # Convert to R1 tensors for fast transfer to device.\n",
        "      images = tf.reshape(images, [-1])\n",
        "    else:\n",
        "      images.set_shape(images.get_shape().merge_with(\n",
        "          tf.TensorShape([batch_size, None, None, None])))\n",
        "      labels.set_shape(labels.get_shape().merge_with(\n",
        "          tf.TensorShape([batch_size, None])))\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "  def mixup(self, batch_size, alpha, images, labels):\n",
        "    \"\"\"Applies Mixup regularization to a batch of images and labels.\n",
        "    [1] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\n",
        "      Mixup: Beyond Empirical Risk Minimization.\n",
        "      ICLR'18, https://arxiv.org/abs/1710.09412\n",
        "    Arguments:\n",
        "      batch_size: The input batch size for images and labels.\n",
        "      alpha: Float that controls the strength of Mixup regularization.\n",
        "      images: A batch of images of shape [batch_size, ...]\n",
        "      labels: A batch of labels of shape [batch_size, num_classes]\n",
        "    Returns:\n",
        "      A tuple of (images, labels) with the same dimensions as the input with\n",
        "      Mixup regularization applied.\n",
        "    \"\"\"\n",
        "    mix_weight = tf.distributions.Beta(alpha, alpha).sample([batch_size, 1])\n",
        "    mix_weight = tf.maximum(mix_weight, 1. - mix_weight)\n",
        "    images_mix_weight = tf.cast(\n",
        "        tf.reshape(mix_weight, [batch_size, 1, 1, 1]), images.dtype)\n",
        "    # Mixup on a single batch is implemented by taking a weighted sum with the\n",
        "    # same batch in reverse.\n",
        "    images_mix = (\n",
        "        images * images_mix_weight + images[::-1] * (1. - images_mix_weight))\n",
        "    labels_mix = labels * mix_weight + labels[::-1] * (1. - mix_weight)\n",
        "    return images_mix, labels_mix\n",
        "\n",
        "  def dataset_parser(self, value):\n",
        "    \"\"\"Parses an image and its label from a serialized ResNet-50 TFExample.\n",
        "    Args:\n",
        "      value: serialized string containing an ImageNet TFExample.\n",
        "    Returns:\n",
        "      Returns a tuple of (image, label) from the TFExample.\n",
        "    \"\"\"\n",
        "    keys_to_features = {\n",
        "        'image/encoded': tf.FixedLenFeature((), tf.string, ''),\n",
        "        'image/class/label': tf.FixedLenFeature([], tf.int64, -1),\n",
        "    }\n",
        "\n",
        "    parsed = tf.parse_single_example(value, keys_to_features)\n",
        "    image_bytes = tf.reshape(parsed['image/encoded'], shape=[])\n",
        "\n",
        "    image = self.image_preprocessing_fn(\n",
        "        image_bytes=image_bytes,\n",
        "        is_training=self.is_training,\n",
        "        image_size=self.image_size,\n",
        "        use_bfloat16=self.use_bfloat16,\n",
        "        augment_name=self.augment_name,\n",
        "        randaug_num_layers=self.randaug_num_layers,\n",
        "        randaug_magnitude=self.randaug_magnitude,\n",
        "        resize_method=self.resize_method)\n",
        "\n",
        "    # The labels will be in range [1,1000], 0 is reserved for background\n",
        "    label = tf.cast(\n",
        "        tf.reshape(parsed['image/class/label'], shape=[]), dtype=tf.int32)\n",
        "\n",
        "    if not self.include_background_label:\n",
        "      # Subtract 1 if the background label is discarded.\n",
        "      label -= 1\n",
        "\n",
        "    onehot_label = tf.one_hot(label, self.num_label_classes)\n",
        "\n",
        "    return image, onehot_label\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def make_source_dataset(self, index, num_hosts):\n",
        "    \"\"\"Makes dataset of serialized TFExamples.\n",
        "    The returned dataset will contain `tf.string` tensors, but these strings are\n",
        "    serialized `TFExample` records that will be parsed by `dataset_parser`.\n",
        "    If self.is_training, the dataset should be infinite.\n",
        "    Args:\n",
        "      index: current host index.\n",
        "      num_hosts: total number of hosts.\n",
        "    Returns:\n",
        "      A `tf.data.Dataset` object.\n",
        "    \"\"\"\n",
        "    return\n",
        "\n",
        "  def input_fn(self, params):\n",
        "    \"\"\"Input function which provides a single batch for train or eval.\n",
        "    Args:\n",
        "      params: `dict` of parameters passed from the `TPUEstimator`.\n",
        "          `params['batch_size']` is always provided and should be used as the\n",
        "          effective batch size.\n",
        "    Returns:\n",
        "      A `tf.data.Dataset` object.\n",
        "    \"\"\"\n",
        "    # Retrieves the batch size for the current shard. The # of shards is\n",
        "    # computed according to the input pipeline deployment. See\n",
        "    # tf.estimator.tpu.RunConfig for details.\n",
        "    batch_size = params['batch_size']\n",
        "\n",
        "    if 'context' in params:\n",
        "      current_host = params['context'].current_input_fn_deployment()[1]\n",
        "      num_hosts = params['context'].num_hosts\n",
        "    else:\n",
        "      current_host = 0\n",
        "      num_hosts = 1\n",
        "\n",
        "    dataset = self.make_source_dataset(current_host, num_hosts)\n",
        "\n",
        "    # Use the fused map-and-batch operation.\n",
        "    #\n",
        "    # For XLA, we must used fixed shapes. Because we repeat the source training\n",
        "    # dataset indefinitely, we can use `drop_remainder=True` to get fixed-size\n",
        "    # batches without dropping any training examples.\n",
        "    #\n",
        "    # When evaluating, `drop_remainder=True` prevents accidentally evaluating\n",
        "    # the same image twice by dropping the final batch if it is less than a full\n",
        "    # batch size. As long as this validation is done with consistent batch size,\n",
        "    # exactly the same images will be used.\n",
        "    dataset = dataset.map(self.dataset_parser, 64).batch(batch_size, True)\n",
        "\n",
        "    # Apply Mixup\n",
        "    if self.is_training and self.mixup_alpha > 0.0:\n",
        "      dataset = dataset.map(\n",
        "          functools.partial(self.mixup, batch_size, self.mixup_alpha),\n",
        "          num_parallel_calls=64)\n",
        "\n",
        "    # Transpose for performance on TPU\n",
        "    if self.transpose_input:\n",
        "      dataset = dataset.map(\n",
        "          lambda images, labels: (tf.transpose(images, [1, 2, 3, 0]), labels),\n",
        "          num_parallel_calls=64)\n",
        "\n",
        "    # Assign static batch size dimension\n",
        "    dataset = dataset.map(functools.partial(self.set_shapes, batch_size), 64)\n",
        "\n",
        "    # Prefetch overlaps in-feed with training\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_deterministic = False\n",
        "    options.experimental_threading.max_intra_op_parallelism = 1\n",
        "    options.experimental_threading.private_threadpool_size = 48\n",
        "    dataset = dataset.with_options(options)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class ImageNetInput(ImageNetTFExampleInput):\n",
        "  \"\"\"Generates ImageNet input_fn from a series of TFRecord files.\n",
        "  The training data is assumed to be in TFRecord format with keys as specified\n",
        "  in the dataset_parser below, sharded across 1024 files, named sequentially:\n",
        "      train-00000-of-01024\n",
        "      train-00001-of-01024\n",
        "      ...\n",
        "      train-01023-of-01024\n",
        "  The validation data is in the same format but sharded in 128 files.\n",
        "  The format of the data required is created by the script at:\n",
        "      https://github.com/tensorflow/tpu/blob/master/tools/datasets/imagenet_to_gcs.py\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               is_training,\n",
        "               use_bfloat16,\n",
        "               transpose_input,\n",
        "               data_dir,\n",
        "               image_size=224,\n",
        "               num_parallel_calls=64,\n",
        "               cache=False,\n",
        "               num_label_classes=1000,\n",
        "               include_background_label=False,\n",
        "               augment_name=None,\n",
        "               mixup_alpha=0.0,\n",
        "               randaug_num_layers=None,\n",
        "               randaug_magnitude=None,\n",
        "               resize_method=None,\n",
        "               holdout_shards=None):\n",
        "    \"\"\"Create an input from TFRecord files.\n",
        "    Args:\n",
        "      is_training: `bool` for whether the input is for training\n",
        "      use_bfloat16: If True, use bfloat16 precision; else use float32.\n",
        "      transpose_input: 'bool' for whether to use the double transpose trick\n",
        "      data_dir: `str` for the directory of the training and validation data;\n",
        "          if 'null' (the literal string 'null') or implicitly False\n",
        "          then construct a null pipeline, consisting of empty images\n",
        "          and blank labels.\n",
        "      image_size: `int` for image size (both width and height).\n",
        "      num_parallel_calls: concurrency level to use when reading data from disk.\n",
        "      cache: if true, fill the dataset by repeating from its cache.\n",
        "      num_label_classes: number of label classes. Default to 1000 for ImageNet.\n",
        "      include_background_label: if true, label #0 is reserved for background.\n",
        "      augment_name: `string` that is the name of the augmentation method\n",
        "          to apply to the image. `autoaugment` if AutoAugment is to be used or\n",
        "          `randaugment` if RandAugment is to be used. If the value is `None` no\n",
        "          no augmentation method will be applied applied. See autoaugment.py\n",
        "          for more details.\n",
        "      mixup_alpha: float to control the strength of Mixup regularization, set\n",
        "          to 0.0 to disable.\n",
        "      randaug_num_layers: 'int', if RandAug is used, what should the number of\n",
        "        layers be. See autoaugment.py for detailed description.\n",
        "      randaug_magnitude: 'int', if RandAug is used, what should the magnitude\n",
        "        be. See autoaugment.py for detailed description.\n",
        "      resize_method: If None, use bicubic in default.\n",
        "      holdout_shards: number of holdout training shards for validation.\n",
        "    \"\"\"\n",
        "    super(ImageNetInput, self).__init__(\n",
        "        is_training=is_training,\n",
        "        image_size=image_size,\n",
        "        use_bfloat16=use_bfloat16,\n",
        "        transpose_input=transpose_input,\n",
        "        num_label_classes=num_label_classes,\n",
        "        include_background_label=include_background_label,\n",
        "        augment_name=augment_name,\n",
        "        mixup_alpha=mixup_alpha,\n",
        "        randaug_num_layers=randaug_num_layers,\n",
        "        randaug_magnitude=randaug_magnitude)\n",
        "    self.data_dir = data_dir\n",
        "    if self.data_dir == 'null' or not self.data_dir:\n",
        "      self.data_dir = None\n",
        "    self.num_parallel_calls = num_parallel_calls\n",
        "    self.cache = cache\n",
        "    self.holdout_shards = holdout_shards\n",
        "\n",
        "  def _get_null_input(self, data):\n",
        "    \"\"\"Returns a null image (all black pixels).\n",
        "    Args:\n",
        "      data: element of a dataset, ignored in this method, since it produces\n",
        "          the same null image regardless of the element.\n",
        "    Returns:\n",
        "      a tensor representing a null image.\n",
        "    \"\"\"\n",
        "    del data  # Unused since output is constant regardless of input\n",
        "    return tf.zeros([self.image_size, self.image_size, 3], tf.bfloat16\n",
        "                    if self.use_bfloat16 else tf.float32)\n",
        "\n",
        "  def dataset_parser(self, value):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    if not self.data_dir:\n",
        "      return value, tf.constant(0., tf.float32, (1000,))\n",
        "    return super(ImageNetInput, self).dataset_parser(value)\n",
        "\n",
        "  def make_source_dataset(self, index, num_hosts):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    if not self.data_dir:\n",
        "      logging.info('Undefined data_dir implies null input')\n",
        "      return tf.data.Dataset.range(1).repeat().map(self._get_null_input)\n",
        "\n",
        "    if self.holdout_shards:\n",
        "      if self.is_training:\n",
        "        filenames = [\n",
        "            os.path.join(self.data_dir, 'train-%05d-of-01024' % i)\n",
        "            for i in range(self.holdout_shards, 1024)\n",
        "        ]\n",
        "      else:\n",
        "        filenames = [\n",
        "            os.path.join(self.data_dir, 'train-%05d-of-01024' % i)\n",
        "            for i in range(0, self.holdout_shards)\n",
        "        ]\n",
        "      for f in filenames[:10]:\n",
        "        logging.info('datafiles: %s', f)\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "    else:\n",
        "      file_pattern = os.path.join(\n",
        "          self.data_dir, 'train-*' if self.is_training else 'validation-*')\n",
        "      logging.info('datafiles: %s', file_pattern)\n",
        "      dataset = tf.data.Dataset.list_files(file_pattern, shuffle=False)\n",
        "\n",
        "    # For multi-host training, we want each hosts to always process the same\n",
        "    # subset of files.  Each host only sees a subset of the entire dataset,\n",
        "    # allowing us to cache larger datasets in memory.\n",
        "    dataset = dataset.shard(num_hosts, index)\n",
        "\n",
        "    if self.is_training and not self.cache:\n",
        "      dataset = dataset.repeat()\n",
        "\n",
        "    def fetch_dataset(filename):\n",
        "      buffer_size = 8 * 1024 * 1024  # 8 MiB per file\n",
        "      dataset = tf.data.TFRecordDataset(filename, buffer_size=buffer_size)\n",
        "      return dataset\n",
        "\n",
        "    # Read the data from disk in parallel\n",
        "    dataset = dataset.interleave(\n",
        "        fetch_dataset, cycle_length=self.num_parallel_calls,\n",
        "        num_parallel_calls=self.num_parallel_calls, deterministic=False)\n",
        "\n",
        "    if self.cache:\n",
        "      dataset = dataset.cache().shuffle(1024 * 16).repeat()\n",
        "    else:\n",
        "      dataset = dataset.shuffle(1024)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Defines a selection of data from a Cloud Bigtable.\n",
        "BigtableSelection = collections.namedtuple('BigtableSelection', [\n",
        "    'project', 'instance', 'table', 'prefix', 'column_family',\n",
        "    'column_qualifier'\n",
        "])\n",
        "\n",
        "\n",
        "class ImageNetBigtableInput(ImageNetTFExampleInput):\n",
        "  \"\"\"Generates ImageNet input_fn from a Bigtable for training or evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               is_training,\n",
        "               use_bfloat16,\n",
        "               transpose_input,\n",
        "               selection,\n",
        "               augment_name=None,\n",
        "               num_label_classes=1000,\n",
        "               include_background_label=False,\n",
        "               mixup_alpha=0.0,\n",
        "               randaug_num_layers=None,\n",
        "               randaug_magnitude=None,\n",
        "               resize_method=None):\n",
        "    \"\"\"Constructs an ImageNet input from a BigtableSelection.\n",
        "    Args:\n",
        "      is_training: `bool` for whether the input is for training\n",
        "      use_bfloat16: If True, use bfloat16 precision; else use float32.\n",
        "      transpose_input: 'bool' for whether to use the double transpose trick\n",
        "      selection: a BigtableSelection specifying a part of a Bigtable.\n",
        "      augment_name: `string` that is the name of the augmentation method\n",
        "          to apply to the image. `autoaugment` if AutoAugment is to be used or\n",
        "          `randaugment` if RandAugment is to be used. If the value is `None` no\n",
        "          no augmentation method will be applied applied. See autoaugment.py\n",
        "          for more details.\n",
        "      num_label_classes: number of label classes. Default to 1000 for ImageNet.\n",
        "      include_background_label: if true, label #0 is reserved for background.\n",
        "      mixup_alpha: float to control the strength of Mixup regularization, set\n",
        "          to 0.0 to disable.\n",
        "      randaug_num_layers: 'int', if RandAug is used, what should the number of\n",
        "        layers be. See autoaugment.py for detailed description.\n",
        "      randaug_magnitude: 'int', if RandAug is used, what should the magnitude\n",
        "        be. See autoaugment.py for detailed description.s\n",
        "      resize_method: if None, use bicubic.\n",
        "    \"\"\"\n",
        "    super(ImageNetBigtableInput, self).__init__(\n",
        "        is_training=is_training,\n",
        "        use_bfloat16=use_bfloat16,\n",
        "        transpose_input=transpose_input,\n",
        "        num_label_classes=num_label_classes,\n",
        "        include_background_label=include_background_label,\n",
        "        augment_name=augment_name,\n",
        "        mixup_alpha=mixup_alpha,\n",
        "        randaug_num_layers=randaug_num_layers,\n",
        "        randaug_magnitude=randaug_magnitude,\n",
        "        resize_method=resize_method)\n",
        "    self.selection = selection\n",
        "\n",
        "  def make_source_dataset(self, index, num_hosts):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    try:\n",
        "      from tensorflow.contrib.cloud import BigtableClient  # pylint: disable=g-import-not-at-top\n",
        "    except ImportError as e:\n",
        "      logging.exception('Bigtable is not supported in TensorFlow 2.x.')\n",
        "      raise e\n",
        "\n",
        "    data = self.selection\n",
        "    client = BigtableClient(data.project, data.instance)\n",
        "    table = client.table(data.table)\n",
        "    ds = table.parallel_scan_prefix(data.prefix,\n",
        "                                    columns=[(data.column_family,\n",
        "                                              data.column_qualifier)])\n",
        "    # The Bigtable datasets will have the shape (row_key, data)\n",
        "    ds_data = ds.map(lambda index, data: data)\n",
        "\n",
        "    if self.is_training:\n",
        "      ds_data = ds_data.repeat()\n",
        "\n",
        "    return ds_data"
      ],
      "metadata": {
        "id": "G_7vaMh9sSQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evulation & Comparision"
      ],
      "metadata": {
        "id": "riy_Qj1_RZNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net=CondConv2D()\n",
        "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "EWM3hmHlspWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(),lr=0.001,momentum=0.9)"
      ],
      "metadata": {
        "id": "4VqPBwwJsr3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    \n",
        "    running_loss= 0.0\n",
        "    for i ,data in enumerate(trainloader, 0):\n",
        "        \n",
        "        inputs, labels =data\n",
        "        device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        inputs, labels =inputs.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs=net(inputs)\n",
        "        loss =criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 ==1999:\n",
        "            print(f'[{epoch+1},{i+1:5d}] loss: {running_loss / 2000 :.3f}')\n",
        "            running_loss =0.0\n",
        "            \n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "mDtgt9z0st2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PATH ='./clfar_net.pth'\n",
        "torch.save(net.state_dict(),PATH)"
      ],
      "metadata": {
        "id": "imPnV8-vsxp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net =Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "outputs = net(images)\n",
        "\n",
        "\n",
        "\n",
        "dataiter =iter(testloader)\n",
        "images , labels =dataiter.next()\n",
        "\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ' ,''.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n"
      ],
      "metadata": {
        "id": "0spv7XuEs1wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred =  {classname: 0 for classname in classes}"
      ],
      "metadata": {
        "id": "QzTv9q_6s3bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = net(images)\n",
        "    _, predictions =torch.max(outputs, 1)\n",
        "\n",
        "    for label, prediction in zip(labels, predictions):\n",
        "      if label ==prediction:\n",
        "        correct_pred[classes[label]] += 1\n",
        "      total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "\n",
        "for classname, correct_count in correct_pred.items():\n",
        "  accuracy =100*float(correct_count) / total_pred[classname]\n",
        "  print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "j3Xa0Dr6s5Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hpoJkep7sxXE"
      }
    }
  ]
}